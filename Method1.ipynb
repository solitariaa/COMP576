{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxYIMqhrSfjp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1528a6cc-95f6-4650-d7ee-2a0ce8a2e60a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CowDEwmEOTZ38IhsEMfhxhPrWcbKZLwD\n",
            "To: /content/truama.xlsx\n",
            "100% 309k/309k [00:00<00:00, 5.96MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1PInO2MlmZRa7Yp7fwBQsJBnWJFuJSZV1\n",
            "To: /content/bloodGlucose.xlsx\n",
            "100% 244k/244k [00:00<00:00, 5.62MB/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install gdown\n",
        "!gdown --id 1CowDEwmEOTZ38IhsEMfhxhPrWcbKZLwD\n",
        "!gdown --id 1PInO2MlmZRa7Yp7fwBQsJBnWJFuJSZV1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load data\n",
        "dataset1 = pd.read_excel(\"bloodGlucose.xlsx\")\n",
        "\n",
        "# Separate static and time-series features\n",
        "static_features_db1 = dataset1[['年龄', '居住地类别', '婚姻状况', '年收入分层', '学历分层', '工作状态', '吸烟分层', '被动吸烟分层', '饮酒分层', '孕前体重', '身高', '孕前BMI']]\n",
        "time_series_features_db1 = dataset1[['weight0', 'glu41', 'weight1', 'glu4b', 'weight2', 'glu4c', 'weight3', 'glu4d', 'weight4']]\n",
        "target_db1 = dataset1['干预']\n",
        "\n",
        "dataset2 = pd.read_excel('truama.xlsx')\n",
        "\n",
        "# Separate Features and Target for Dataset 2\n",
        "static_features_db2 = dataset2[['年龄', '居住地类别', '婚姻状况', '年收入分层', '学历分层',\n",
        "                                 '工作状态', '吸烟分层', '被动吸烟分层', '饮酒分层', '孕前体重',\n",
        "                                 '身高', '孕前BMI', '意外怀孕', '助孕治疗', '已生孩子数分层',\n",
        "                                 'ACE1', 'ACE2', 'ACE3', 'ACE4', 'ACE5', 'ACE6', 'ACE7', 'ACE8',\n",
        "                                 'ACE9', 'ACE10', 'F1a', 'F2a', 'F3a', 'F4a', 'F5a', 'F6a',\n",
        "                                 'F7a', 'F8a', 'F9a', 'F10a', 'F11a', 'F12a', 'F13a', 'F14a',\n",
        "                                 'F15a', 'F16a', 'F17a', 'F18a', 'F19a']]\n",
        "target_db2 = dataset2['干预']"
      ],
      "metadata": {
        "id": "V4RH_gYASh6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Standarization\n",
        "scaler = StandardScaler()\n",
        "\n",
        "static_features_db1[['年龄', '身高', '孕前BMI', '孕前体重']] = scaler.fit_transform(static_features_db1[['年龄', '身高', '孕前BMI', '孕前体重']])\n",
        "static_features_db2[['年龄', '身高', '孕前BMI', '孕前体重']] = scaler.fit_transform(static_features_db2[['年龄', '身高', '孕前BMI', '孕前体重']])"
      ],
      "metadata": {
        "id": "9zfRcsSaSkEn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8839821a-361a-4829-b976-51e317e823f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-95f3ca595cf9>:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  static_features_db1[['年龄', '身高', '孕前BMI', '孕前体重']] = scaler.fit_transform(static_features_db1[['年龄', '身高', '孕前BMI', '孕前体重']])\n",
            "<ipython-input-3-95f3ca595cf9>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  static_features_db2[['年龄', '身高', '孕前BMI', '孕前体重']] = scaler.fit_transform(static_features_db2[['年龄', '身高', '孕前BMI', '孕前体重']])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install dtaidistance"
      ],
      "metadata": {
        "id": "-9-ErDDlSkjq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6536f8ca-480b-4a52-90ae-e323d898c10d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dtaidistance in /usr/local/lib/python3.10/dist-packages (2.3.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from dtaidistance) (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method 1"
      ],
      "metadata": {
        "id": "hJiCUzFymFVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 用了BCEWithLogitsLoss(接受 logits 作为输入，而不是 Sigmoid 激活后的概率) + pos_weight(), ReduceLROnPlateau Scheduler\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import numpy as np\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from sklearn.metrics import recall_score, precision_score, fbeta_score, accuracy_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.functional import sigmoid\n",
        "from torch.utils.data import Subset\n",
        "def find_best_threshold(outputs, targets, beta=2, min_threshold=0.1):\n",
        "    precision, recall, thresholds = precision_recall_curve(targets, outputs)\n",
        "\n",
        "    # Compute F-beta score for thresholds\n",
        "    fbeta_scores = (1 + beta**2) * (precision[:-1] * recall[:-1]) / ((beta**2 * precision[:-1]) + recall[:-1] + 1e-5)\n",
        "\n",
        "    # Filter out thresholds that are too low\n",
        "    valid_idx = thresholds >= min_threshold\n",
        "    thresholds = thresholds[valid_idx]\n",
        "    fbeta_scores = fbeta_scores[valid_idx]\n",
        "\n",
        "    if len(fbeta_scores) == 0:\n",
        "        # Fallback in case all thresholds are filtered out\n",
        "        return min_threshold, 0.0\n",
        "\n",
        "    best_idx = np.argmax(fbeta_scores)\n",
        "    return thresholds[best_idx], fbeta_scores[best_idx]\n",
        "# Shared features\n",
        "shared_features_db2 = static_features_db2[['年龄', '居住地类别', '婚姻状况', '年收入分层', '学历分层',\n",
        "                                 '工作状态', '吸烟分层', '被动吸烟分层', '饮酒分层', '孕前体重',\n",
        "                                 '身高', '孕前BMI']].to_numpy()\n",
        "# Dataset 2-specific features\n",
        "specific_features_db2 = static_features_db2[['意外怀孕', '助孕治疗', '已生孩子数分层',\n",
        "                                 'ACE1', 'ACE2', 'ACE3', 'ACE4', 'ACE5', 'ACE6', 'ACE7', 'ACE8',\n",
        "                                 'ACE9', 'ACE10', 'F1a', 'F2a', 'F3a', 'F4a', 'F5a', 'F6a',\n",
        "                                 'F7a', 'F8a', 'F9a', 'F10a', 'F11a', 'F12a', 'F13a', 'F14a',\n",
        "                                 'F15a', 'F16a', 'F17a', 'F18a', 'F19a']].to_numpy()\n",
        "shared_features_db1 = static_features_db1.to_numpy()\n",
        "targets_db1 = target_db1.to_numpy()\n",
        "targets_db2 = target_db2.to_numpy()\n",
        "# 对共享特征进行SMOTE采样\n",
        "smote = SMOTE(random_state=42)\n",
        "\n",
        "# 对共享特征和目标进行重采样\n",
        "shared_features_resampled_db1, targets_resampled_db1 = smote.fit_resample(shared_features_db1, targets_db1)\n",
        "shared_features_resampled_db2, targets_resampled_db2 = smote.fit_resample(shared_features_db2, targets_db2)\n",
        "\n",
        "\n",
        "# 获取重采样后的索引 (仅适用于带索引的输入数据)\n",
        "# 假设 shared_features_db1 和 shared_features_db2 的最后一列是原始索引\n",
        "resampled_indices_db1 = shared_features_resampled_db1[:, -1].astype(int)  # 从最后一列提取索引\n",
        "\n",
        "resampled_indices_db2 = shared_features_resampled_db2[:, -1].astype(int)\n",
        "\n",
        "\n",
        "# 对独特特征进行重采样\n",
        "specific_features_resampled_db2, _ = smote.fit_resample(specific_features_db2, targets_db2)\n",
        "\n",
        "# Masking\n",
        "glu_features = time_series_features_db1[['glu41', 'glu4b', 'glu4c', 'glu4d']].replace([' ', ''], np.nan).infer_objects(copy=False).to_numpy()\n",
        "weight_features = time_series_features_db1[['weight0', 'weight1', 'weight2', 'weight3', 'weight4']].replace([' ', ''], np.nan).infer_objects(copy=False).to_numpy()\n",
        "glu_features = np.nan_to_num(glu_features.astype(np.float32), nan=0.0)\n",
        "weight_features = np.nan_to_num(weight_features.astype(np.float32), nan=0.0)\n",
        "# Reshape to (samples, channels, time_steps)\n",
        "glu_features = glu_features.reshape(glu_features.shape[0], 4, 1)  # (samples, time_steps, input_size)\n",
        "weight_features = weight_features.reshape(weight_features.shape[0], 5, 1)\n",
        "\n",
        "\n",
        "# 对时间序列特征进行对齐 (基于 resampled_indices_db1)\n",
        "glu_features_resampled = glu_features[resampled_indices_db1]\n",
        "weight_features_resampled = weight_features[resampled_indices_db1]\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, shared_features, glu_features=None, weight_features=None, unique_features=None, targets=None):\n",
        "        \"\"\"\n",
        "        初始化数据集\n",
        "        :param shared_features: 共享特征 (numpy array)\n",
        "        :param glu_features: 时间序列特征 glu (numpy array, 可选)\n",
        "        :param weight_features: 时间序列特征 weight (numpy array, 可选)\n",
        "        :param unique_features: 独特特征 (numpy array, 可选)\n",
        "        :param targets: 标签 (numpy array)\n",
        "        \"\"\"\n",
        "        self.shared_features = torch.tensor(shared_features, dtype=torch.float32)\n",
        "        self.glu_features = (\n",
        "            torch.tensor(glu_features, dtype=torch.float32) if glu_features is not None else None\n",
        "        )\n",
        "        self.weight_features = (\n",
        "            torch.tensor(weight_features, dtype=torch.float32) if weight_features is not None else None\n",
        "        )\n",
        "        self.unique_features = (\n",
        "            torch.tensor(unique_features, dtype=torch.float32) if unique_features is not None else None\n",
        "        )\n",
        "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.shared_features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = {\"shared_features\": self.shared_features[idx]}\n",
        "        if self.glu_features is not None:\n",
        "            sample[\"glu_features\"] = self.glu_features[idx]\n",
        "        if self.weight_features is not None:\n",
        "            sample[\"weight_features\"] = self.weight_features[idx]\n",
        "        if self.unique_features is not None:\n",
        "            sample[\"unique_features\"] = self.unique_features[idx]\n",
        "        sample[\"target\"] = self.targets[idx]\n",
        "        return sample\n",
        "\n",
        "\n",
        "db1_train_idx, db1_test_idx = train_test_split(\n",
        "    np.arange(len(targets_resampled_db1)),\n",
        "    test_size=0.1,\n",
        "    stratify=targets_resampled_db1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "db2_train_idx, db2_test_idx = train_test_split(\n",
        "    np.arange(len(targets_resampled_db2)),\n",
        "    test_size=0.1,\n",
        "    stratify=targets_resampled_db2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 使用 CustomDataset 实例创建完整数据集\n",
        "db1_dataset = CustomDataset(\n",
        "    shared_features=shared_features_resampled_db1,\n",
        "    glu_features=glu_features_resampled,\n",
        "    weight_features=weight_features_resampled,\n",
        "    targets=targets_resampled_db1\n",
        ")\n",
        "\n",
        "db2_dataset = CustomDataset(\n",
        "    shared_features=shared_features_resampled_db2,\n",
        "    unique_features=specific_features_resampled_db2,\n",
        "    targets=targets_resampled_db2\n",
        ")\n",
        "\n",
        "# 基于分层采样的索引创建训练集和测试集的 Subset\n",
        "db1_train_dataset = Subset(db1_dataset, db1_train_idx)\n",
        "db1_test_dataset = Subset(db1_dataset, db1_test_idx)\n",
        "\n",
        "db2_train_dataset = Subset(db2_dataset, db2_train_idx)\n",
        "db2_test_dataset = Subset(db2_dataset, db2_test_idx)\n",
        "\n",
        "# 创建 DataLoader\n",
        "db1_train_loader = DataLoader(db1_train_dataset, batch_size=32, shuffle=True)\n",
        "db1_test_loader = DataLoader(db1_test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "db2_train_loader = DataLoader(db2_train_dataset, batch_size=32, shuffle=True)\n",
        "db2_test_loader = DataLoader(db2_test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "class SharedFeatureModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SharedFeatureModel, self).__init__()\n",
        "        self.shared_branch = nn.Sequential(\n",
        "            nn.Linear(12, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # LSTM branch for glu_features\n",
        "        self.glu_lstm = nn.LSTM(input_size=1, hidden_size=64, num_layers=2, batch_first=True)\n",
        "        self.glu_bn = nn.BatchNorm1d(64)  # Batch Normalization for LSTM outputs\n",
        "        self.glu_fc = nn.Sequential(\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # LSTM branch for weight_features\n",
        "        self.weight_lstm = nn.LSTM(input_size=1, hidden_size=64, num_layers=2, batch_first=True)\n",
        "        self.weight_bn = nn.BatchNorm1d(64)  # Batch Normalization for LSTM outputs\n",
        "        self.weight_fc = nn.Sequential(\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.db2_branch = nn.Sequential(\n",
        "            nn.Linear(32, 64),  # 12 是 unique_features_db2 的输入维度\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, 32),  # 输出维度为 32\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "\n",
        "        # 推迟 fc 初始化\n",
        "        self.fc = None\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, shared_features, glu_features=None, weight_features=None, unique_features_db2=None, db=\"db1\"):\n",
        "        shared_output = self.shared_branch(shared_features)\n",
        "        outputs = [shared_output]\n",
        "\n",
        "        if glu_features is not None:\n",
        "            glu_lstm_out, _ = self.glu_lstm(glu_features)\n",
        "            glu_lstm_out = glu_lstm_out[:, -1, :]\n",
        "            glu_bn_out = self.glu_bn(glu_lstm_out)\n",
        "            glu_output = self.glu_fc(glu_bn_out)\n",
        "            outputs.append(glu_output)\n",
        "\n",
        "        if weight_features is not None:\n",
        "            weight_lstm_out, _ = self.weight_lstm(weight_features)\n",
        "            weight_lstm_out = weight_lstm_out[:, -1, :]\n",
        "            weight_bn_out = self.weight_bn(weight_lstm_out)\n",
        "            weight_output = self.weight_fc(weight_bn_out)\n",
        "            outputs.append(weight_output)\n",
        "\n",
        "        if db == \"db2\" and unique_features_db2 is not None:\n",
        "            unique_output = self.db2_branch(unique_features_db2)\n",
        "            outputs.append(unique_output)\n",
        "\n",
        "        combined = torch.cat(outputs, dim=1)\n",
        "\n",
        "        if self.fc is None or combined.shape[1] != self.fc[0].in_features:\n",
        "            total_features = combined.shape[1]\n",
        "            self.fc = nn.Sequential(\n",
        "                nn.Linear(total_features, 64),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.3),\n",
        "                nn.Linear(64, 1)  # No Sigmoid here\n",
        "            )\n",
        "\n",
        "\n",
        "        output = self.fc(combined)\n",
        "        return output\n",
        "\n",
        "\n",
        "# Calculate pos_weight for db1\n",
        "num_positive_db1 = targets_resampled_db1.sum()\n",
        "num_negative_db1 = len(targets_resampled_db1) - num_positive_db1\n",
        "pos_weight_db1 = torch.tensor([num_negative_db1 / num_positive_db1], dtype=torch.float32)\n",
        "\n",
        "# Calculate pos_weight for db2\n",
        "num_positive_db2 = targets_resampled_db2.sum()\n",
        "num_negative_db2 = len(targets_resampled_db2) - num_positive_db2\n",
        "pos_weight_db2 = torch.tensor([num_negative_db2 / num_positive_db2], dtype=torch.float32)\n",
        "\n",
        "\n",
        "# 初始化模型\n",
        "shared_feature_dim = shared_features_resampled_db1.shape[1]  # 共享特征维度\n",
        "db2_unique_feature_dim = specific_features_resampled_db2.shape[1]  # db2独特特征维度\n",
        "\n",
        "\n",
        "model = SharedFeatureModel()\n",
        "# Use pos_weight for db1 and db2\n",
        "criterion_db1 = nn.BCEWithLogitsLoss(pos_weight=pos_weight_db1)\n",
        "criterion_db2 = nn.BCEWithLogitsLoss(pos_weight=pos_weight_db2)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
        "def train_model(model, db1_train_loader, db1_test_loader, db2_train_loader, db2_test_loader,\n",
        "                            optimizer, criterion_db1, criterion_db2, epochs=100, beta=2, min_threshold=0, switch_interval=1):\n",
        "    \"\"\"\n",
        "    交替训练模型并在每个 epoch 后评估\n",
        "    :param model: 共享模型\n",
        "    :param db1_train_loader: 数据集 db1 的训练数据加载器\n",
        "    :param db1_test_loader: 数据集 db1 的测试数据加载器\n",
        "    :param db2_train_loader: 数据集 db2 的训练数据加载器\n",
        "    :param db2_test_loader: 数据集 db2 的测试数据加载器\n",
        "    :param optimizer: 优化器\n",
        "    :param criterion_db1: 数据集 db1 的损失函数\n",
        "    :param criterion_db2: 数据集 db2 的损失函数\n",
        "    :param epochs: 总训练轮数\n",
        "    :param beta: F-beta 分数的 beta 值\n",
        "    :param min_threshold: 最小阈值\n",
        "    :param switch_interval: 多少个 epoch 切换一次数据集\n",
        "    \"\"\"\n",
        "    for epoch in range(epochs):\n",
        "        # 决定当前使用的数据集\n",
        "        if (epoch // switch_interval) % 2 == 0:\n",
        "            train_loader = db1_train_loader\n",
        "            test_loader = db1_test_loader\n",
        "            criterion = criterion_db1\n",
        "            db = \"db1\"\n",
        "        else:\n",
        "            train_loader = db2_train_loader\n",
        "            test_loader = db2_test_loader\n",
        "            criterion = criterion_db2\n",
        "            db = \"db2\"\n",
        "\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        for batch in train_loader:\n",
        "            shared_features = batch[\"shared_features\"]\n",
        "            glu_features = batch.get(\"glu_features\", None)\n",
        "            weight_features = batch.get(\"weight_features\", None)\n",
        "            unique_features = batch.get(\"unique_features\", None)\n",
        "            targets = batch[\"target\"].unsqueeze(1)  # (N,) -> (N, 1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 模型前向传播\n",
        "            outputs = model(\n",
        "                shared_features=shared_features,\n",
        "                glu_features=glu_features,\n",
        "                weight_features=weight_features,\n",
        "                unique_features_db2=unique_features,\n",
        "                db=db\n",
        "            )\n",
        "\n",
        "            # 计算损失\n",
        "            loss = criterion(outputs, targets)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # 反向传播和优化\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # 评估模型\n",
        "        model.eval()\n",
        "        all_targets = []\n",
        "        all_outputs = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                shared_features = batch[\"shared_features\"]\n",
        "                glu_features = batch.get(\"glu_features\", None)\n",
        "                weight_features = batch.get(\"weight_features\", None)\n",
        "                unique_features = batch.get(\"unique_features\", None)\n",
        "                targets = batch[\"target\"].unsqueeze(1)  # (N,) -> (N, 1)\n",
        "\n",
        "                # 模型前向传播\n",
        "                outputs = model(\n",
        "                    shared_features=shared_features,\n",
        "                    glu_features=glu_features,\n",
        "                    weight_features=weight_features,\n",
        "                    unique_features_db2=unique_features,\n",
        "                    db=db\n",
        "                )\n",
        "\n",
        "                # 收集预测值和真实标签\n",
        "                all_targets.extend(targets.cpu().numpy())\n",
        "                all_outputs.extend(outputs.cpu().numpy())\n",
        "\n",
        "        # 计算评估指标\n",
        "        all_targets = np.array(all_targets).flatten()\n",
        "        all_outputs = np.array(all_outputs).flatten()\n",
        "\n",
        "        best_threshold, best_fbeta = find_best_threshold(all_outputs, all_targets, beta=beta, min_threshold=min_threshold)\n",
        "\n",
        "        predictions = (all_outputs >= best_threshold).astype(int)\n",
        "        precision = precision_score(all_targets, predictions)\n",
        "        recall = recall_score(all_targets, predictions)\n",
        "        f2_score = fbeta_score(all_targets, predictions, beta=beta)\n",
        "        accuracy = accuracy_score(all_targets, predictions)\n",
        "\n",
        "        print(\n",
        "            f\"Epoch [{epoch+1}/{epochs}] ({db}), Loss: {total_loss/len(train_loader):.4f}, \"\n",
        "            f\"Best Threshold: {best_threshold:.4f}, Recall: {recall:.4f}, \"\n",
        "            f\"Precision: {precision:.4f}, F{beta} Score: {f2_score:.4f}, Accuracy: {accuracy:.4f}\"\n",
        "        )\n",
        "\n",
        "\n",
        "print(\"Alternating training on db1 and db2...\")\n",
        "train_model(\n",
        "    model,\n",
        "    db1_train_loader, db1_test_loader,\n",
        "    db2_train_loader, db2_test_loader,\n",
        "    optimizer, criterion_db1, criterion_db2,\n",
        "    epochs=100, switch_interval=1\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "Yxlw_x19v5bl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "715e651a-f1f6-4d55-831e-20b4b4f1dc99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-b1a1d0704268>:65: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  glu_features = time_series_features_db1[['glu41', 'glu4b', 'glu4c', 'glu4d']].replace([' ', ''], np.nan).infer_objects(copy=False).to_numpy()\n",
            "<ipython-input-5-b1a1d0704268>:66: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  weight_features = time_series_features_db1[['weight0', 'weight1', 'weight2', 'weight3', 'weight4']].replace([' ', ''], np.nan).infer_objects(copy=False).to_numpy()\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alternating training on db1 and db2...\n",
            "Epoch [1/100] (db1), Loss: 0.6934, Best Threshold: 0.0233, Recall: 0.0341, Precision: 0.7500, F2 Score: 0.0421, Accuracy: 0.5114\n",
            "Epoch [2/100] (db2), Loss: 0.6968, Best Threshold: 0.0085, Recall: 0.0349, Precision: 0.6000, F2 Score: 0.0430, Accuracy: 0.5058\n",
            "Epoch [3/100] (db1), Loss: 0.6885, Best Threshold: 0.0017, Recall: 0.2159, Precision: 0.6552, F2 Score: 0.2493, Accuracy: 0.5511\n",
            "Epoch [4/100] (db2), Loss: 0.6928, Best Threshold: 0.0029, Recall: 0.8256, Precision: 0.5338, F2 Score: 0.7442, Accuracy: 0.5523\n",
            "Epoch [5/100] (db1), Loss: 0.6845, Best Threshold: 0.0086, Recall: 0.2727, Precision: 0.6154, F2 Score: 0.3069, Accuracy: 0.5511\n",
            "Epoch [6/100] (db2), Loss: 0.7032, Best Threshold: 0.0024, Recall: 0.8023, Precision: 0.5188, F2 Score: 0.7233, Accuracy: 0.5291\n",
            "Epoch [7/100] (db1), Loss: 0.6867, Best Threshold: 0.0016, Recall: 0.4886, Precision: 0.7049, F2 Score: 0.5206, Accuracy: 0.6420\n",
            "Epoch [8/100] (db2), Loss: 0.6975, Best Threshold: 0.0015, Recall: 0.3372, Precision: 0.6591, F2 Score: 0.3737, Accuracy: 0.5814\n",
            "Epoch [9/100] (db1), Loss: 0.6880, Best Threshold: 0.0009, Recall: 0.1932, Precision: 0.7083, F2 Score: 0.2261, Accuracy: 0.5568\n",
            "Epoch [10/100] (db2), Loss: 0.6853, Best Threshold: 0.0067, Recall: 0.4302, Precision: 0.6066, F2 Score: 0.4568, Accuracy: 0.5756\n",
            "Epoch [11/100] (db1), Loss: 0.6959, Best Threshold: 0.0024, Recall: 0.8295, Precision: 0.5573, F2 Score: 0.7557, Accuracy: 0.5852\n",
            "Epoch [12/100] (db2), Loss: 0.6986, Best Threshold: 0.0007, Recall: 0.1512, Precision: 0.6842, F2 Score: 0.1791, Accuracy: 0.5407\n",
            "Epoch [13/100] (db1), Loss: 0.6955, Best Threshold: 0.0574, Recall: 0.2045, Precision: 0.7500, F2 Score: 0.2394, Accuracy: 0.5682\n",
            "Epoch [14/100] (db2), Loss: 0.6868, Best Threshold: 0.0024, Recall: 0.5233, Precision: 0.6081, F2 Score: 0.5383, Accuracy: 0.5930\n",
            "Epoch [15/100] (db1), Loss: 0.6848, Best Threshold: 0.0167, Recall: 0.6364, Precision: 0.5957, F2 Score: 0.6278, Accuracy: 0.6023\n",
            "Epoch [16/100] (db2), Loss: 0.6819, Best Threshold: 0.0070, Recall: 0.8372, Precision: 0.5294, F2 Score: 0.7500, Accuracy: 0.5465\n",
            "Epoch [17/100] (db1), Loss: 0.6904, Best Threshold: 0.0507, Recall: 0.2045, Precision: 0.6923, F2 Score: 0.2381, Accuracy: 0.5568\n",
            "Epoch [18/100] (db2), Loss: 0.6798, Best Threshold: 0.0049, Recall: 0.4651, Precision: 0.6780, F2 Score: 0.4963, Accuracy: 0.6221\n",
            "Epoch [19/100] (db1), Loss: 0.6851, Best Threshold: 0.0088, Recall: 0.6364, Precision: 0.6292, F2 Score: 0.6349, Accuracy: 0.6307\n",
            "Epoch [20/100] (db2), Loss: 0.6906, Best Threshold: 0.0001, Recall: 0.6860, Precision: 0.5619, F2 Score: 0.6570, Accuracy: 0.5756\n",
            "Epoch [21/100] (db1), Loss: 0.6834, Best Threshold: 0.0029, Recall: 0.5455, Precision: 0.6575, F2 Score: 0.5647, Accuracy: 0.6307\n",
            "Epoch [22/100] (db2), Loss: 0.6962, Best Threshold: 0.0099, Recall: 0.8605, Precision: 0.5211, F2 Score: 0.7613, Accuracy: 0.5349\n",
            "Epoch [23/100] (db1), Loss: 0.6955, Best Threshold: 0.0078, Recall: 0.7273, Precision: 0.5565, F2 Score: 0.6852, Accuracy: 0.5739\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [24/100] (db2), Loss: 0.7171, Best Threshold: 0.0000, Recall: 0.0000, Precision: 0.0000, F2 Score: 0.0000, Accuracy: 0.5000\n",
            "Epoch [25/100] (db1), Loss: 0.6906, Best Threshold: 0.0007, Recall: 0.7500, Precision: 0.5366, F2 Score: 0.6947, Accuracy: 0.5511\n",
            "Epoch [26/100] (db2), Loss: 0.7035, Best Threshold: 0.0016, Recall: 0.2791, Precision: 0.4800, F2 Score: 0.3046, Accuracy: 0.4884\n",
            "Epoch [27/100] (db1), Loss: 0.6945, Best Threshold: 0.0068, Recall: 0.2386, Precision: 0.7000, F2 Score: 0.2749, Accuracy: 0.5682\n",
            "Epoch [28/100] (db2), Loss: 0.6797, Best Threshold: 0.0110, Recall: 0.8605, Precision: 0.5692, F2 Score: 0.7806, Accuracy: 0.6047\n",
            "Epoch [29/100] (db1), Loss: 0.6976, Best Threshold: 0.0379, Recall: 0.0682, Precision: 0.6667, F2 Score: 0.0831, Accuracy: 0.5170\n",
            "Epoch [30/100] (db2), Loss: 0.7191, Best Threshold: 0.0094, Recall: 0.9651, Precision: 0.4940, F2 Score: 0.8105, Accuracy: 0.4884\n",
            "Epoch [31/100] (db1), Loss: 0.6937, Best Threshold: 0.0355, Recall: 0.1932, Precision: 0.7391, F2 Score: 0.2267, Accuracy: 0.5625\n",
            "Epoch [32/100] (db2), Loss: 0.7015, Best Threshold: 0.0028, Recall: 0.1163, Precision: 0.9091, F2 Score: 0.1408, Accuracy: 0.5523\n",
            "Epoch [33/100] (db1), Loss: 0.6935, Best Threshold: 0.0006, Recall: 0.8182, Precision: 0.5333, F2 Score: 0.7392, Accuracy: 0.5511\n",
            "Epoch [34/100] (db2), Loss: 0.7030, Best Threshold: 0.0064, Recall: 0.3372, Precision: 0.5577, F2 Score: 0.3662, Accuracy: 0.5349\n",
            "Epoch [35/100] (db1), Loss: 0.6902, Best Threshold: 0.0075, Recall: 0.2955, Precision: 0.6667, F2 Score: 0.3325, Accuracy: 0.5739\n",
            "Epoch [36/100] (db2), Loss: 0.6981, Best Threshold: 0.0015, Recall: 0.0581, Precision: 0.4167, F2 Score: 0.0702, Accuracy: 0.4884\n",
            "Epoch [37/100] (db1), Loss: 0.6960, Best Threshold: 0.0038, Recall: 0.1591, Precision: 0.7000, F2 Score: 0.1882, Accuracy: 0.5455\n",
            "Epoch [38/100] (db2), Loss: 0.6866, Best Threshold: 0.0019, Recall: 0.4186, Precision: 0.6792, F2 Score: 0.4534, Accuracy: 0.6105\n",
            "Epoch [39/100] (db1), Loss: 0.6968, Best Threshold: 0.0184, Recall: 0.1591, Precision: 0.7368, F2 Score: 0.1887, Accuracy: 0.5511\n",
            "Epoch [40/100] (db2), Loss: 0.6954, Best Threshold: 0.0022, Recall: 0.1628, Precision: 0.5833, F2 Score: 0.1902, Accuracy: 0.5233\n",
            "Epoch [41/100] (db1), Loss: 0.6887, Best Threshold: 0.0003, Recall: 0.4886, Precision: 0.5658, F2 Score: 0.5023, Accuracy: 0.5568\n",
            "Epoch [42/100] (db2), Loss: 0.6989, Best Threshold: 0.0099, Recall: 0.8488, Precision: 0.4932, F2 Score: 0.7419, Accuracy: 0.4884\n",
            "Epoch [43/100] (db1), Loss: 0.6853, Best Threshold: 0.0161, Recall: 0.2386, Precision: 0.7241, F2 Score: 0.2756, Accuracy: 0.5739\n",
            "Epoch [44/100] (db2), Loss: 0.6871, Best Threshold: 0.0044, Recall: 0.1977, Precision: 0.6296, F2 Score: 0.2291, Accuracy: 0.5407\n",
            "Epoch [45/100] (db1), Loss: 0.6938, Best Threshold: 0.0101, Recall: 0.1705, Precision: 0.7143, F2 Score: 0.2011, Accuracy: 0.5511\n",
            "Epoch [46/100] (db2), Loss: 0.6943, Best Threshold: 0.0003, Recall: 0.2093, Precision: 0.6000, F2 Score: 0.2406, Accuracy: 0.5349\n",
            "Epoch [47/100] (db1), Loss: 0.6933, Best Threshold: 0.0059, Recall: 0.2159, Precision: 0.7308, F2 Score: 0.2513, Accuracy: 0.5682\n",
            "Epoch [48/100] (db2), Loss: 0.6817, Best Threshold: 0.0004, Recall: 0.3023, Precision: 0.7222, F2 Score: 0.3421, Accuracy: 0.5930\n",
            "Epoch [49/100] (db1), Loss: 0.6904, Best Threshold: 0.0006, Recall: 0.1818, Precision: 0.6957, F2 Score: 0.2133, Accuracy: 0.5511\n",
            "Epoch [50/100] (db2), Loss: 0.7191, Best Threshold: 0.1205, Recall: 1.0000, Precision: 0.5000, F2 Score: 0.8333, Accuracy: 0.5000\n",
            "Epoch [51/100] (db1), Loss: 0.6910, Best Threshold: 0.0240, Recall: 0.0227, Precision: 1.0000, F2 Score: 0.0282, Accuracy: 0.5114\n",
            "Epoch [52/100] (db2), Loss: 0.7243, Best Threshold: 0.1080, Recall: 1.0000, Precision: 0.5000, F2 Score: 0.8333, Accuracy: 0.5000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [53/100] (db1), Loss: 0.6969, Best Threshold: 0.0000, Recall: 0.0000, Precision: 0.0000, F2 Score: 0.0000, Accuracy: 0.5000\n",
            "Epoch [54/100] (db2), Loss: 0.6926, Best Threshold: 0.1095, Recall: 0.9419, Precision: 0.5827, F2 Score: 0.8385, Accuracy: 0.6337\n",
            "Epoch [55/100] (db1), Loss: 0.6940, Best Threshold: 0.0165, Recall: 0.2614, Precision: 0.6970, F2 Score: 0.2987, Accuracy: 0.5739\n",
            "Epoch [56/100] (db2), Loss: 0.6911, Best Threshold: 0.0117, Recall: 0.5116, Precision: 0.5641, F2 Score: 0.5213, Accuracy: 0.5581\n",
            "Epoch [57/100] (db1), Loss: 0.6939, Best Threshold: 0.0023, Recall: 0.4545, Precision: 0.5634, F2 Score: 0.4728, Accuracy: 0.5511\n",
            "Epoch [58/100] (db2), Loss: 0.6882, Best Threshold: 0.0010, Recall: 0.2791, Precision: 0.6486, F2 Score: 0.3150, Accuracy: 0.5640\n",
            "Epoch [59/100] (db1), Loss: 0.6830, Best Threshold: 0.0042, Recall: 0.4432, Precision: 0.6724, F2 Score: 0.4756, Accuracy: 0.6136\n",
            "Epoch [60/100] (db2), Loss: 0.6918, Best Threshold: 0.0059, Recall: 0.2558, Precision: 0.7857, F2 Score: 0.2957, Accuracy: 0.5930\n",
            "Epoch [61/100] (db1), Loss: 0.6907, Best Threshold: 0.0216, Recall: 0.1250, Precision: 0.7333, F2 Score: 0.1499, Accuracy: 0.5398\n",
            "Epoch [62/100] (db2), Loss: 0.6923, Best Threshold: 0.0003, Recall: 0.6628, Precision: 0.5135, F2 Score: 0.6264, Accuracy: 0.5174\n",
            "Epoch [63/100] (db1), Loss: 0.6994, Best Threshold: 0.0014, Recall: 0.0000, Precision: 0.0000, F2 Score: 0.0000, Accuracy: 0.4886\n",
            "Epoch [64/100] (db2), Loss: 0.7034, Best Threshold: 0.0011, Recall: 0.6395, Precision: 0.5189, F2 Score: 0.6111, Accuracy: 0.5233\n",
            "Epoch [65/100] (db1), Loss: 0.6930, Best Threshold: 0.0007, Recall: 0.3182, Precision: 0.5957, F2 Score: 0.3509, Accuracy: 0.5511\n",
            "Epoch [66/100] (db2), Loss: 0.6827, Best Threshold: 0.0008, Recall: 0.8488, Precision: 0.5748, F2 Score: 0.7749, Accuracy: 0.6105\n",
            "Epoch [67/100] (db1), Loss: 0.6938, Best Threshold: 0.0017, Recall: 0.8977, Precision: 0.5338, F2 Score: 0.7900, Accuracy: 0.5568\n",
            "Epoch [68/100] (db2), Loss: 0.7074, Best Threshold: 0.0042, Recall: 0.8953, Precision: 0.5238, F2 Score: 0.7841, Accuracy: 0.5407\n",
            "Epoch [69/100] (db1), Loss: 0.6893, Best Threshold: 0.0006, Recall: 0.5000, Precision: 0.6197, F2 Score: 0.5201, Accuracy: 0.5966\n",
            "Epoch [70/100] (db2), Loss: 0.6901, Best Threshold: 0.0079, Recall: 0.4070, Precision: 0.5147, F2 Score: 0.4248, Accuracy: 0.5116\n",
            "Epoch [71/100] (db1), Loss: 0.7037, Best Threshold: 0.0009, Recall: 0.4318, Precision: 0.4935, F2 Score: 0.4429, Accuracy: 0.4943\n",
            "Epoch [72/100] (db2), Loss: 0.6906, Best Threshold: 0.0006, Recall: 0.7674, Precision: 0.5789, F2 Score: 0.7205, Accuracy: 0.6047\n",
            "Epoch [73/100] (db1), Loss: 0.6868, Best Threshold: 0.0023, Recall: 0.2727, Precision: 0.6667, F2 Score: 0.3093, Accuracy: 0.5682\n",
            "Epoch [74/100] (db2), Loss: 0.6906, Best Threshold: 0.0061, Recall: 0.1744, Precision: 0.6250, F2 Score: 0.2038, Accuracy: 0.5349\n",
            "Epoch [75/100] (db1), Loss: 0.6923, Best Threshold: 0.0039, Recall: 0.3864, Precision: 0.6538, F2 Score: 0.4208, Accuracy: 0.5909\n",
            "Epoch [76/100] (db2), Loss: 0.6928, Best Threshold: 0.0043, Recall: 0.4419, Precision: 0.6552, F2 Score: 0.4726, Accuracy: 0.6047\n",
            "Epoch [77/100] (db1), Loss: 0.6840, Best Threshold: 0.0040, Recall: 0.2955, Precision: 0.6500, F2 Score: 0.3316, Accuracy: 0.5682\n",
            "Epoch [78/100] (db2), Loss: 0.7027, Best Threshold: 0.0017, Recall: 0.4419, Precision: 0.6333, F2 Score: 0.4703, Accuracy: 0.5930\n",
            "Epoch [79/100] (db1), Loss: 0.6928, Best Threshold: 0.0019, Recall: 0.5000, Precision: 0.6567, F2 Score: 0.5251, Accuracy: 0.6193\n",
            "Epoch [80/100] (db2), Loss: 0.6893, Best Threshold: 0.0030, Recall: 0.5349, Precision: 0.5750, F2 Score: 0.5425, Accuracy: 0.5698\n",
            "Epoch [81/100] (db1), Loss: 0.6929, Best Threshold: 0.0101, Recall: 0.1250, Precision: 0.6875, F2 Score: 0.1495, Accuracy: 0.5341\n",
            "Epoch [82/100] (db2), Loss: 0.7017, Best Threshold: 0.0188, Recall: 0.2093, Precision: 0.6429, F2 Score: 0.2419, Accuracy: 0.5465\n",
            "Epoch [83/100] (db1), Loss: 0.6944, Best Threshold: 0.0007, Recall: 0.3864, Precision: 0.6182, F2 Score: 0.4177, Accuracy: 0.5739\n",
            "Epoch [84/100] (db2), Loss: 0.6973, Best Threshold: 0.0025, Recall: 0.0581, Precision: 0.5000, F2 Score: 0.0706, Accuracy: 0.5000\n",
            "Epoch [85/100] (db1), Loss: 0.6942, Best Threshold: 0.0042, Recall: 0.0114, Precision: 0.3333, F2 Score: 0.0141, Accuracy: 0.4943\n",
            "Epoch [86/100] (db2), Loss: 0.6920, Best Threshold: 0.0003, Recall: 0.5116, Precision: 0.6027, F2 Score: 0.5276, Accuracy: 0.5872\n",
            "Epoch [87/100] (db1), Loss: 0.6942, Best Threshold: 0.0335, Recall: 0.2159, Precision: 0.7600, F2 Score: 0.2520, Accuracy: 0.5739\n",
            "Epoch [88/100] (db2), Loss: 0.6917, Best Threshold: 0.0024, Recall: 0.4302, Precision: 0.5068, F2 Score: 0.4436, Accuracy: 0.5058\n",
            "Epoch [89/100] (db1), Loss: 0.6913, Best Threshold: 0.0002, Recall: 0.7273, Precision: 0.6737, F2 Score: 0.7159, Accuracy: 0.6875\n",
            "Epoch [90/100] (db2), Loss: 0.6874, Best Threshold: 0.0045, Recall: 0.3140, Precision: 0.6136, F2 Score: 0.3479, Accuracy: 0.5581\n",
            "Epoch [91/100] (db1), Loss: 0.6873, Best Threshold: 0.0050, Recall: 0.4545, Precision: 0.7692, F2 Score: 0.4950, Accuracy: 0.6591\n",
            "Epoch [92/100] (db2), Loss: 0.6943, Best Threshold: 0.0069, Recall: 0.1163, Precision: 0.7143, F2 Score: 0.1397, Accuracy: 0.5349\n",
            "Epoch [93/100] (db1), Loss: 0.6992, Best Threshold: 0.0524, Recall: 1.0000, Precision: 0.5087, F2 Score: 0.8381, Accuracy: 0.5170\n",
            "Epoch [94/100] (db2), Loss: 0.6983, Best Threshold: 0.0007, Recall: 0.8837, Precision: 0.5033, F2 Score: 0.7677, Accuracy: 0.5058\n",
            "Epoch [95/100] (db1), Loss: 0.7042, Best Threshold: 0.0382, Recall: 1.0000, Precision: 0.5301, F2 Score: 0.8494, Accuracy: 0.5568\n",
            "Epoch [96/100] (db2), Loss: 0.6932, Best Threshold: 0.0046, Recall: 0.7442, Precision: 0.5614, F2 Score: 0.6987, Accuracy: 0.5814\n",
            "Epoch [97/100] (db1), Loss: 0.6910, Best Threshold: 0.0096, Recall: 0.2386, Precision: 0.7241, F2 Score: 0.2756, Accuracy: 0.5739\n",
            "Epoch [98/100] (db2), Loss: 0.6899, Best Threshold: 0.0125, Recall: 0.9651, Precision: 0.5220, F2 Score: 0.8250, Accuracy: 0.5407\n",
            "Epoch [99/100] (db1), Loss: 0.7001, Best Threshold: 0.0002, Recall: 0.5909, Precision: 0.5361, F2 Score: 0.5791, Accuracy: 0.5398\n",
            "Epoch [100/100] (db2), Loss: 0.6913, Best Threshold: 0.0550, Recall: 0.9302, Precision: 0.5556, F2 Score: 0.8197, Accuracy: 0.5930\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for creating the new test set."
      ],
      "metadata": {
        "id": "luM7viIOEI7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 提取重叠特征\n",
        "shared_features = ['年龄', '居住地类别', '婚姻状况', '年收入分层', '学历分层', '工作状态', '吸烟分层', '被动吸烟分层', '饮酒分层', '孕前体重', '身高', '孕前BMI']\n",
        "\n",
        "# 重置索引\n",
        "db1_shared = dataset1[shared_features]\n",
        "db2_shared = dataset2[shared_features]\n",
        "db1_targets = dataset1['干预']\n",
        "db2_targets = dataset2['干预']\n",
        "\n",
        "db1_shared = db1_shared.reset_index(drop=True)\n",
        "db2_shared = db2_shared.reset_index(drop=True)\n",
        "db1_targets = db1_targets.reset_index(drop=True)\n",
        "db2_targets = db2_targets.reset_index(drop=True)\n",
        "\n",
        "# 提取独有特征（包括时间序列特征）\n",
        "db1_unique_features = dataset1.drop(columns=shared_features + ['干预'])\n",
        "db2_unique_features = dataset2.drop(columns=shared_features + ['干预'])\n",
        "\n",
        "# 设置共享特征的相似阈值\n",
        "similarity_threshold = 1e-3\n",
        "\n",
        "# 创建新测试集\n",
        "new_test_data = []\n",
        "\n",
        "for i, row_db1 in db1_shared.iterrows():\n",
        "    for j, row_db2 in db2_shared.iterrows():\n",
        "        # 判断共享特征的相似性\n",
        "        if np.allclose(row_db1.values, row_db2.values, atol=similarity_threshold):\n",
        "            if db1_targets.iloc[i] == db2_targets.iloc[j]:\n",
        "                # 默认重叠部分用 db2 的值\n",
        "                combined_row = row_db2.to_dict()  # 转为字典，避免索引冲突\n",
        "\n",
        "                # 补充 db1 和 db2 的独有特征\n",
        "                db1_non_shared = db1_unique_features.iloc[i].to_dict()\n",
        "                db2_non_shared = db2_unique_features.iloc[j].to_dict()\n",
        "\n",
        "                # 将所有特征合并\n",
        "                combined_row.update(db1_non_shared)  # 添加 db1 独有特征\n",
        "                combined_row.update(db2_non_shared)  # 添加 db2 独有特征\n",
        "\n",
        "                # 添加目标值\n",
        "                combined_row['target'] = db1_targets.iloc[i]  # 使用相同的目标值\n",
        "\n",
        "                # 追加到新测试集\n",
        "                new_test_data.append(combined_row)\n",
        "\n",
        "                # 匹配成功后立即跳出内循环\n",
        "                break\n",
        "\n",
        "# 将新测试集转换为 DataFrame\n",
        "new_test_df = pd.DataFrame(new_test_data)\n",
        "\n",
        "# 保存到 Excel 文件\n",
        "output_file_path = \"new_test_dataset_with_all_features.xlsx\"\n",
        "new_test_df.to_excel(output_file_path, index=False)\n",
        "\n",
        "# 提供下载\n",
        "from google.colab import files\n",
        "files.download(output_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "nR0tN7DMWtw_",
        "outputId": "f3962c0b-d4bc-4fff-f5c9-881c3135051d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9b44f246-9e49-4dd4-96a7-bc224f6c5287\", \"new_test_dataset_with_all_features.xlsx\", 238203)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}