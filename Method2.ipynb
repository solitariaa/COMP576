{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown\n",
        "!gdown --id 1CowDEwmEOTZ38IhsEMfhxhPrWcbKZLwD\n",
        "!gdown --id 1PInO2MlmZRa7Yp7fwBQsJBnWJFuJSZV1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rB4a1sultTdX",
        "outputId": "ba17ec81-bf02-4918-b584-25fe801eaaba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CowDEwmEOTZ38IhsEMfhxhPrWcbKZLwD\n",
            "To: /content/truama.xlsx\n",
            "100% 309k/309k [00:00<00:00, 67.5MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1PInO2MlmZRa7Yp7fwBQsJBnWJFuJSZV1\n",
            "To: /content/bloodGlucose.xlsx\n",
            "100% 244k/244k [00:00<00:00, 39.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load data\n",
        "dataset1 = pd.read_excel(\"bloodGlucose.xlsx\")\n",
        "\n",
        "# Separate static and time-series features\n",
        "static_features_db1 = dataset1[['年龄', '居住地类别', '婚姻状况', '年收入分层', '学历分层', '工作状态', '吸烟分层', '被动吸烟分层', '饮酒分层', '孕前体重', '身高', '孕前BMI']]\n",
        "time_series_features_db1 = dataset1[['weight0', 'glu41', 'weight1', 'glu4b', 'weight2', 'glu4c', 'weight3', 'glu4d', 'weight4']]\n",
        "target_db1 = dataset1['干预']\n",
        "\n",
        "dataset2 = pd.read_excel('truama.xlsx')\n",
        "\n",
        "# Separate Features and Target for Dataset 2\n",
        "static_features_db2 = dataset2[['年龄', '居住地类别', '婚姻状况', '年收入分层', '学历分层',\n",
        "                                 '工作状态', '吸烟分层', '被动吸烟分层', '饮酒分层', '孕前体重',\n",
        "                                 '身高', '孕前BMI', '意外怀孕', '助孕治疗', '已生孩子数分层',\n",
        "                                 'ACE1', 'ACE2', 'ACE3', 'ACE4', 'ACE5', 'ACE6', 'ACE7', 'ACE8',\n",
        "                                 'ACE9', 'ACE10', 'F1a', 'F2a', 'F3a', 'F4a', 'F5a', 'F6a',\n",
        "                                 'F7a', 'F8a', 'F9a', 'F10a', 'F11a', 'F12a', 'F13a', 'F14a',\n",
        "                                 'F15a', 'F16a', 'F17a', 'F18a', 'F19a']]\n",
        "target_db2 = dataset2['干预']"
      ],
      "metadata": {
        "id": "P4FH7_wkHl1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 创建标准化器\n",
        "scaler_static_db1 = StandardScaler()\n",
        "scaler_static_db2 = StandardScaler()\n",
        "\n",
        "# 对训练数据进行标准化\n",
        "static_features_db1[['年龄', '身高', '孕前BMI', '孕前体重']] = scaler_static_db1.fit_transform(static_features_db1[['年龄', '身高', '孕前BMI', '孕前体重']])\n",
        "static_features_db2[['年龄', '身高', '孕前BMI', '孕前体重']] = scaler_static_db2.fit_transform(static_features_db2[['年龄', '身高', '孕前BMI', '孕前体重']])\n",
        "\n",
        "# 保存标准化参数\n",
        "mean_db1 = scaler_static_db1.mean_\n",
        "scale_db1 = scaler_static_db1.scale_\n",
        "\n",
        "mean_db2 = scaler_static_db2.mean_\n",
        "scale_db2 = scaler_static_db2.scale_\n",
        "\n",
        "print(\"Saved Mean DB1:\", mean_db1)\n",
        "print(\"Saved Scale DB1:\", scale_db1)\n",
        "print(\"Saved Mean DB2:\", mean_db2)\n",
        "print(\"Saved Scale DB2:\", scale_db2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zY36sC1TLti2",
        "outputId": "22501519-9363-4eaa-c4b3-b583f82a63ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved Mean DB1: [ 28.79748609 161.70849453  21.67057728  56.72085786]\n",
            "Saved Scale DB1: [4.06999718 4.74138094 3.3589254  9.49922943]\n",
            "Saved Mean DB2: [ 28.68893222 161.81691312  21.63959573  56.72005545]\n",
            "Saved Scale DB2: [4.00608356 4.7094961  3.35030079 9.51160468]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-88688edc4f72>:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  static_features_db1[['年龄', '身高', '孕前BMI', '孕前体重']] = scaler_static_db1.fit_transform(static_features_db1[['年龄', '身高', '孕前BMI', '孕前体重']])\n",
            "<ipython-input-3-88688edc4f72>:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  static_features_db2[['年龄', '身高', '孕前BMI', '孕前体重']] = scaler_static_db2.fit_transform(static_features_db2[['年龄', '身高', '孕前BMI', '孕前体重']])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install dtaidistance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMBfJrJmCnvF",
        "outputId": "5cbc9ab2-4ed7-47bd-9a35-c0f1c64869ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dtaidistance\n",
            "  Downloading dtaidistance-2.3.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from dtaidistance) (1.26.4)\n",
            "Downloading dtaidistance-2.3.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dtaidistance\n",
            "Successfully installed dtaidistance-2.3.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dtw\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import recall_score, precision_score, fbeta_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from dtaidistance import dtw\n",
        "import numpy as np\n",
        "\n",
        "# Function to fill missing values using DTW\n",
        "def fill_nan_with_dtw(sequence, complete_sequences):\n",
        "    \"\"\"\n",
        "    Replace np.nan in a sequence using DTW alignment with complete sequences.\n",
        "\n",
        "    :param sequence: (array) The sequence containing np.nan\n",
        "    :param complete_sequences: (list of arrays) List of complete sequences for alignment\n",
        "    :return: (array) Sequence with np.nan replaced\n",
        "    \"\"\"\n",
        "    # Identify missing value indices\n",
        "    nan_indices = np.where(np.isnan(sequence))[0]\n",
        "\n",
        "    # If no missing values, return the sequence as is\n",
        "    if len(nan_indices) == 0:\n",
        "        return sequence\n",
        "\n",
        "    # Find the best-matching sequence using DTW\n",
        "    best_match = None\n",
        "    best_distance = float('inf')\n",
        "    for seq in complete_sequences:\n",
        "        # Compute DTW distance, ignoring np.nan in the sequence\n",
        "        distance = dtw.distance(sequence[~np.isnan(sequence)], seq[:len(sequence)][~np.isnan(sequence)])\n",
        "        if distance < best_distance:\n",
        "            best_distance = distance\n",
        "            best_match = seq\n",
        "\n",
        "    # Fill missing values using the best-matching sequence\n",
        "    if best_match is not None:\n",
        "        filled_sequence = np.copy(sequence)\n",
        "        for nan_index in nan_indices:\n",
        "            filled_sequence[nan_index] = best_match[nan_index]\n",
        "        return filled_sequence\n",
        "\n",
        "    return sequence\n",
        "\n",
        "# Add indices to static features\n",
        "static_features_db1[\"index\"] = static_features_db1.index\n",
        "static_features_with_index = static_features_db1.to_numpy()\n",
        "targets = target_db1.to_numpy()\n",
        "\n",
        "static_features_db2[\"index\"] = static_features_db2.index\n",
        "static_features_with_index_db2 = static_features_db2.to_numpy()\n",
        "targets_db2 = target_db2.to_numpy()\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "static_features_resampled, targets_resampled = smote.fit_resample(static_features_with_index, targets)\n",
        "resampled_indices = static_features_resampled[:, -1].astype(int)  # Extract the index column\n",
        "static_features_resampled = static_features_resampled[:, :-1]\n",
        "\n",
        "static_features_resampled_db2, targets_resampled_db2 = smote.fit_resample(static_features_with_index_db2, targets_db2)\n",
        "resampled_indices_db2 = static_features_resampled_db2[:, -1].astype(int)  # Extract the index column\n",
        "static_features_resampled_db2 = static_features_resampled_db2[:, :-1]\n",
        "\n",
        "glu_features = time_series_features_db1[['glu41', 'glu4b', 'glu4c', 'glu4d']].replace([' ', ''], np.nan).to_numpy()\n",
        "weight_features = time_series_features_db1[['weight0', 'weight1', 'weight2', 'weight3', 'weight4']].replace([' ', ''], np.nan).to_numpy()\n",
        "\n",
        "# DTW\n",
        "# Extract complete sequences (no np.nan values)\n",
        "complete_glu_sequences = [seq for seq in glu_features if not np.any(np.isnan(seq))]\n",
        "complete_weight_sequences = [seq for seq in weight_features if not np.any(np.isnan(seq))]\n",
        "# Fill missing values in glu_features\n",
        "glu_features_filled = np.array([\n",
        "    fill_nan_with_dtw(seq, complete_glu_sequences) for seq in glu_features\n",
        "])\n",
        "# Fill missing values in weight_features\n",
        "weight_features_filled = np.array([\n",
        "    fill_nan_with_dtw(seq, complete_weight_sequences) for seq in weight_features\n",
        "])\n",
        "glu_features_filled = np.nan_to_num(glu_features_filled.astype(np.float32), nan=0.0)\n",
        "glu_features = glu_features_filled.reshape(glu_features_filled.shape[0], 4, 1)\n",
        "weight_features = weight_features_filled.reshape(weight_features_filled.shape[0], 5, 1)\n",
        "\n",
        "\n",
        "# Align time-series data using the resampled indices\n",
        "glu_features_resampled = glu_features[resampled_indices]\n",
        "weight_features_resampled = weight_features[resampled_indices]\n",
        "\n",
        "\n",
        "# Split Data into Training and Testing Sets\n",
        "\n",
        "# Split static, glu, weight, and targets into training and testing sets\n",
        "X_static_train_db1, X_static_test_db1, X_glu_train, X_glu_test, X_weight_train, X_weight_test, y_train_db1, y_test_db1 = train_test_split(\n",
        "    static_features_resampled, glu_features_resampled, weight_features_resampled, targets_resampled, test_size=0.1, random_state=42, stratify=targets_resampled\n",
        ")\n",
        "\n",
        "X_static_train_db2, X_static_test_db2, y_train_db2, y_test_db2 = train_test_split(\n",
        "    static_features_resampled_db2, targets_resampled_db2, test_size=0.1, random_state=42, stratify=targets_resampled_db2\n",
        ")\n",
        "\n",
        "# Print data shapes to verify\n",
        "print(\"Dataset 1 Static Training Shape:\", X_static_train_db1.shape)\n",
        "print(\"Dataset 1 Glu Training Shape:\", X_glu_train.shape)\n",
        "print(\"Dataset 1 Weight Training Shape:\", X_weight_train.shape)\n",
        "print(\"Dataset 2 Static Training Shape:\", X_static_train_db2.shape)\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, static_data, glu_data, weight_data, target):\n",
        "        # Handle pandas DataFrame or NumPy array\n",
        "        if isinstance(static_data, pd.DataFrame):\n",
        "            self.static_data = torch.tensor(static_data.to_numpy(), dtype=torch.float32)\n",
        "        else:\n",
        "            self.static_data = torch.tensor(static_data, dtype=torch.float32)\n",
        "\n",
        "        self.glu_data = torch.tensor(glu_data, dtype=torch.float32)  # glu_data is already NumPy\n",
        "        self.weight_data = torch.tensor(weight_data, dtype=torch.float32)  # weight_data is already NumPy\n",
        "        self.target = torch.tensor(target, dtype=torch.float32)  # Convert to tensor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.target)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"static\": self.static_data[idx],\n",
        "            \"glu\": self.glu_data[idx],\n",
        "            \"weight\": self.weight_data[idx],\n",
        "            \"target\": self.target[idx],\n",
        "        }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWnvUW0yJwIw",
        "outputId": "83373ef9-29a6-4693-ce7f-2b2d43796d71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-99ef90a92b84>:49: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  static_features_db1[\"index\"] = static_features_db1.index\n",
            "<ipython-input-5-99ef90a92b84>:53: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  static_features_db2[\"index\"] = static_features_db2.index\n",
            "<ipython-input-5-99ef90a92b84>:66: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  glu_features = time_series_features_db1[['glu41', 'glu4b', 'glu4c', 'glu4d']].replace([' ', ''], np.nan).to_numpy()\n",
            "<ipython-input-5-99ef90a92b84>:67: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  weight_features = time_series_features_db1[['weight0', 'weight1', 'weight2', 'weight3', 'weight4']].replace([' ', ''], np.nan).to_numpy()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset 1 Static Training Shape: (1584, 12)\n",
            "Dataset 1 Glu Training Shape: (1584, 4, 1)\n",
            "Dataset 1 Weight Training Shape: (1584, 5, 1)\n",
            "Dataset 2 Static Training Shape: (1548, 44)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 正常Masking版本\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import recall_score, precision_score, fbeta_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Add indices to static features\n",
        "static_features_db1[\"index\"] = static_features_db1.index\n",
        "static_features_with_index = static_features_db1.to_numpy()\n",
        "targets = target_db1.to_numpy()\n",
        "\n",
        "static_features_db2[\"index\"] = static_features_db2.index\n",
        "static_features_with_index_db2 = static_features_db2.to_numpy()\n",
        "targets_db2 = target_db2.to_numpy()\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "static_features_resampled, targets_resampled = smote.fit_resample(static_features_with_index, targets)\n",
        "resampled_indices = static_features_resampled[:, -1].astype(int)  # Extract the index column\n",
        "static_features_resampled = static_features_resampled[:, :-1]\n",
        "\n",
        "static_features_resampled_db2, targets_resampled_db2 = smote.fit_resample(static_features_with_index_db2, targets_db2)\n",
        "resampled_indices_db2 = static_features_resampled_db2[:, -1].astype(int)  # Extract the index column\n",
        "static_features_resampled_db2 = static_features_resampled_db2[:, :-1]\n",
        "\n",
        "glu_features = time_series_features_db1[['glu41', 'glu4b', 'glu4c', 'glu4d']].replace([' ', ''], np.nan).to_numpy()\n",
        "weight_features = time_series_features_db1[['weight0', 'weight1', 'weight2', 'weight3', 'weight4']].replace([' ', ''], np.nan).to_numpy()\n",
        "\n",
        "\n",
        "glu_features = np.nan_to_num(glu_features.astype(np.float32), nan=0.0)\n",
        "weight_features = np.nan_to_num(weight_features.astype(np.float32), nan=0.0)\n",
        "\n",
        "\n",
        "# Reshape to (samples, time_steps, features)\n",
        "glu_features = glu_features.reshape(glu_features.shape[0], 4, 1)\n",
        "weight_features = weight_features.reshape(weight_features.shape[0], 5, 1)\n",
        "\n",
        "# Align time-series data using the resampled indices\n",
        "glu_features_resampled = glu_features[resampled_indices]\n",
        "weight_features_resampled = weight_features[resampled_indices]\n",
        "\n",
        "\n",
        "# Split Data into Training and Testing Sets\n",
        "\n",
        "# Split static, glu, weight, and targets into training and testing sets\n",
        "X_static_train_db1, X_static_test_db1, X_glu_train, X_glu_test, X_weight_train, X_weight_test, y_train_db1, y_test_db1 = train_test_split(\n",
        "    static_features_resampled, glu_features_resampled, weight_features_resampled, targets_resampled, test_size=0.1, random_state=42, stratify=targets_resampled\n",
        ")\n",
        "X_static_train_db2, X_static_test_db2, y_train_db2, y_test_db2 = train_test_split(\n",
        "    static_features_resampled_db2, targets_resampled_db2, test_size=0.1, random_state=42, stratify=targets_resampled_db2\n",
        ")\n",
        "\n",
        "# Print data shapes to verify\n",
        "print(\"Dataset 1 Static Training Shape:\", X_static_train_db1.shape)\n",
        "print(\"Dataset 1 Glu Training Shape:\", X_glu_train.shape)\n",
        "print(\"Dataset 1 Weight Training Shape:\", X_weight_train.shape)\n",
        "print(\"Dataset 2 Static Training Shape:\", X_static_train_db2.shape)\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, static_data, glu_data, weight_data, target):\n",
        "        # Handle pandas DataFrame or NumPy array\n",
        "        if isinstance(static_data, pd.DataFrame):\n",
        "            self.static_data = torch.tensor(static_data.to_numpy(), dtype=torch.float32)\n",
        "        else:\n",
        "            self.static_data = torch.tensor(static_data, dtype=torch.float32)\n",
        "\n",
        "        self.glu_data = torch.tensor(glu_data, dtype=torch.float32)  # glu_data is already NumPy\n",
        "        self.weight_data = torch.tensor(weight_data, dtype=torch.float32)  # weight_data is already NumPy\n",
        "        self.target = torch.tensor(target, dtype=torch.float32)  # Convert to tensor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.target)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"static\": self.static_data[idx],\n",
        "            \"glu\": self.glu_data[idx],\n",
        "            \"weight\": self.weight_data[idx],\n",
        "            \"target\": self.target[idx],\n",
        "        }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "es3zB0xzuq0G",
        "outputId": "674f9f49-a588-4cf0-b8aa-33917efa2eb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset 1 Static Training Shape: (1584, 12)\n",
            "Dataset 1 Glu Training Shape: (1584, 4, 1)\n",
            "Dataset 1 Weight Training Shape: (1584, 5, 1)\n",
            "Dataset 2 Static Training Shape: (1548, 44)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-366d10ee84ba>:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  static_features_db1[\"index\"] = static_features_db1.index\n",
            "<ipython-input-55-366d10ee84ba>:16: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  static_features_db2[\"index\"] = static_features_db2.index\n",
            "<ipython-input-55-366d10ee84ba>:29: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  glu_features = time_series_features_db1[['glu41', 'glu4b', 'glu4c', 'glu4d']].replace([' ', ''], np.nan).to_numpy()\n",
            "<ipython-input-55-366d10ee84ba>:30: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  weight_features = time_series_features_db1[['weight0', 'weight1', 'weight2', 'weight3', 'weight4']].replace([' ', ''], np.nan).to_numpy()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dynamic threshold & 无scheduler\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import numpy as np\n",
        "\n",
        "def find_best_threshold(outputs, targets, beta=2, min_threshold=0.1):\n",
        "    precision, recall, thresholds = precision_recall_curve(targets, outputs)\n",
        "\n",
        "    # Compute F-beta score for thresholds\n",
        "    fbeta_scores = (1 + beta**2) * (precision[:-1] * recall[:-1]) / ((beta**2 * precision[:-1]) + recall[:-1] + 1e-5)\n",
        "\n",
        "    # Filter out thresholds that are too low\n",
        "    valid_idx = thresholds >= min_threshold\n",
        "    thresholds = thresholds[valid_idx]\n",
        "    fbeta_scores = fbeta_scores[valid_idx]\n",
        "\n",
        "    if len(fbeta_scores) == 0:\n",
        "        # Fallback in case all thresholds are filtered out\n",
        "        return min_threshold, 0.0\n",
        "\n",
        "    best_idx = np.argmax(fbeta_scores)\n",
        "    return thresholds[best_idx], fbeta_scores[best_idx]\n",
        "\n",
        "\n",
        "train_dataset_db1 = CustomDataset(X_static_train_db1, X_glu_train, X_weight_train, y_train_db1)\n",
        "test_dataset_db1 = CustomDataset(X_static_test_db1, X_glu_test, X_weight_test, y_test_db1)\n",
        "\n",
        "train_loader_db1 = DataLoader(train_dataset_db1, batch_size=32, shuffle=True)\n",
        "test_loader_db1 = DataLoader(test_dataset_db1, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the Multi-Input Model\n",
        "class MultiInputModel(nn.Module):\n",
        "    def __init__(self, static_input_dim, glu_input_dim, weight_input_dim):\n",
        "        super(MultiInputModel, self).__init__()\n",
        "\n",
        "        # Static feature branch\n",
        "        self.static_fc = nn.Sequential(\n",
        "            nn.Linear(static_input_dim, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # LSTM branch for glu with attention and masking\n",
        "        self.glu_lstm = nn.LSTM(input_size=1, hidden_size=64, num_layers=2, batch_first=True)\n",
        "        self.glu_attention = nn.Sequential(\n",
        "            nn.Linear(64, 1),  # Attention score for each time step\n",
        "            nn.Softmax(dim=1)  # Normalize scores across valid time steps\n",
        "        )\n",
        "        self.glu_fc = nn.Sequential(\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # LSTM branch for weight with attention and masking\n",
        "        self.weight_lstm = nn.LSTM(input_size=1, hidden_size=64, num_layers=2, batch_first=True)\n",
        "        self.weight_attention = nn.Sequential(\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "        self.weight_fc = nn.Sequential(\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Combined high-level features\n",
        "        self.combined_fc = nn.Sequential(\n",
        "            nn.Linear(32 + 32 + 32, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, static_input, glu_input, weight_input):\n",
        "        # Static feature branch\n",
        "        static_out = self.static_fc(static_input)\n",
        "\n",
        "        # Glu branch with masking and attention\n",
        "        glu_mask = (glu_input.sum(dim=-1) != 0).float()  # Mask for valid time steps\n",
        "        glu_out, _ = self.glu_lstm(glu_input)  # Output shape: (batch_size, seq_len, hidden_size)\n",
        "\n",
        "        attention_scores_glu = self.glu_attention(glu_out).squeeze(-1)  # Attention scores: (batch_size, seq_len)\n",
        "        attention_scores_glu = attention_scores_glu * glu_mask  # Apply mask to attention scores\n",
        "        attention_weights_glu = attention_scores_glu / (attention_scores_glu.sum(dim=1, keepdim=True) + 1e-8)  # Normalize\n",
        "\n",
        "        glu_out = torch.bmm(attention_weights_glu.unsqueeze(1), glu_out).squeeze(1)  # Weighted sum: (batch_size, hidden_size)\n",
        "        glu_out = self.glu_fc(glu_out)\n",
        "\n",
        "        # Weight branch with masking and attention\n",
        "        weight_mask = (weight_input.sum(dim=-1) != 0).float()  # Mask for valid time steps\n",
        "        weight_out, _ = self.weight_lstm(weight_input)\n",
        "\n",
        "        attention_scores_weight = self.weight_attention(weight_out).squeeze(-1)  # Attention scores: (batch_size, seq_len)\n",
        "        attention_scores_weight = attention_scores_weight * weight_mask  # Apply mask to attention scores\n",
        "        attention_weights_weight = attention_scores_weight / (attention_scores_weight.sum(dim=1, keepdim=True) + 1e-8)  # Normalize\n",
        "\n",
        "        weight_out = torch.bmm(attention_weights_weight.unsqueeze(1), weight_out).squeeze(1)  # Weighted sum: (batch_size, hidden_size)\n",
        "        weight_out = self.weight_fc(weight_out)\n",
        "\n",
        "        # Combine all branches\n",
        "        combined = torch.cat([static_out, glu_out, weight_out], dim=1)\n",
        "        output = self.combined_fc(combined)\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "# Initialize the Model, Loss, and Optimizer\n",
        "static_input_dim = X_static_train_db1.shape[1]\n",
        "glu_input_dim = X_glu_train.shape[1]\n",
        "weight_input_dim = X_weight_train.shape[1]\n",
        "print(static)\n",
        "model = MultiInputModel(static_input_dim, glu_input_dim, weight_input_dim)\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for binary classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "# Training Model\n",
        "def train_model(model, train_loader, test_loader, criterion, optimizer, epochs=50):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for batch in train_loader:\n",
        "            static_input = batch[\"static\"]\n",
        "            glu_input = batch[\"glu\"]\n",
        "            weight_input = batch[\"weight\"]\n",
        "            target = batch[\"target\"].unsqueeze(1)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(static_input, glu_input, weight_input)\n",
        "            loss = criterion(outputs, target)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Evaluate model and find the best threshold\n",
        "        outputs, targets, best_threshold, recall, precision, f2_score, accuracy = evaluate_model(model, test_loader)\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {running_loss / len(train_loader):.4f}, \"\n",
        "              f\"Best Threshold: {best_threshold:.4f}, Recall: {recall:.4f}, Precision: {precision:.4f}, \"\n",
        "              f\"F2 Score: {f2_score:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "def calculate_metrics(outputs, targets, threshold=0.5):\n",
        "    predictions = (outputs > threshold).astype(int)\n",
        "    targets = targets.astype(int)\n",
        "    recall = recall_score(targets, predictions)\n",
        "    precision = precision_score(targets, predictions)\n",
        "    f2_score = fbeta_score(targets, predictions, beta=2)\n",
        "    accuracy = accuracy_score(targets, predictions)\n",
        "    return recall, precision, f2_score, accuracy\n",
        "\n",
        "# Evaluation\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    all_outputs = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            static_input = batch[\"static\"]\n",
        "            glu_input = batch[\"glu\"]\n",
        "            weight_input = batch[\"weight\"]\n",
        "            target = batch[\"target\"]\n",
        "\n",
        "            outputs = model(static_input, glu_input, weight_input)\n",
        "            all_outputs.append(outputs.squeeze().numpy())\n",
        "            all_targets.append(target.numpy())\n",
        "\n",
        "    outputs = np.concatenate(all_outputs)\n",
        "    targets = np.concatenate(all_targets)\n",
        "\n",
        "    # Find the best dynamic threshold\n",
        "    best_threshold, best_fbeta = find_best_threshold(outputs, targets, beta=2)\n",
        "\n",
        "    # Apply the best threshold\n",
        "    predictions = (outputs > best_threshold).astype(int)\n",
        "    recall = recall_score(targets, predictions)\n",
        "    precision = precision_score(targets, predictions)\n",
        "    f2_score = fbeta_score(targets, predictions, beta=2)\n",
        "    accuracy = accuracy_score(targets, predictions)\n",
        "\n",
        "    return outputs, targets, best_threshold, recall, precision, f2_score, accuracy\n",
        "\n",
        "\n",
        "train_model(model, train_loader_db1, test_loader_db1, criterion, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtkpmoyfQ5yB",
        "outputId": "fd903837-f900-4401-eb70-7b2ea4221e9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 0.6950, Best Threshold: 0.4575, Recall: 0.9773, Precision: 0.5309, F2 Score: 0.8366, Accuracy: 0.5568\n",
            "Epoch [2/50], Loss: 0.6777, Best Threshold: 0.3426, Recall: 0.9773, Precision: 0.5276, F2 Score: 0.8350, Accuracy: 0.5511\n",
            "Epoch [3/50], Loss: 0.6287, Best Threshold: 0.1805, Recall: 0.9773, Precision: 0.5409, F2 Score: 0.8415, Accuracy: 0.5739\n",
            "Epoch [4/50], Loss: 0.6107, Best Threshold: 0.2142, Recall: 0.9886, Precision: 0.5273, F2 Score: 0.8414, Accuracy: 0.5511\n",
            "Epoch [5/50], Loss: 0.5800, Best Threshold: 0.1587, Recall: 0.9886, Precision: 0.5273, F2 Score: 0.8414, Accuracy: 0.5511\n",
            "Epoch [6/50], Loss: 0.5505, Best Threshold: 0.1592, Recall: 0.9886, Precision: 0.5305, F2 Score: 0.8430, Accuracy: 0.5568\n",
            "Epoch [7/50], Loss: 0.5359, Best Threshold: 0.1242, Recall: 0.9886, Precision: 0.5337, F2 Score: 0.8447, Accuracy: 0.5625\n",
            "Epoch [8/50], Loss: 0.5141, Best Threshold: 0.1420, Recall: 0.9886, Precision: 0.5241, F2 Score: 0.8398, Accuracy: 0.5455\n",
            "Epoch [9/50], Loss: 0.5072, Best Threshold: 0.1222, Recall: 0.9886, Precision: 0.5179, F2 Score: 0.8365, Accuracy: 0.5341\n",
            "Epoch [10/50], Loss: 0.4920, Best Threshold: 0.1156, Recall: 0.9886, Precision: 0.5404, F2 Score: 0.8480, Accuracy: 0.5739\n",
            "Epoch [11/50], Loss: 0.5032, Best Threshold: 0.1019, Recall: 0.9773, Precision: 0.5375, F2 Score: 0.8398, Accuracy: 0.5682\n",
            "Epoch [12/50], Loss: 0.4535, Best Threshold: 0.1221, Recall: 0.9886, Precision: 0.5404, F2 Score: 0.8480, Accuracy: 0.5739\n",
            "Epoch [13/50], Loss: 0.4534, Best Threshold: 0.1259, Recall: 0.9773, Precision: 0.5443, F2 Score: 0.8431, Accuracy: 0.5795\n",
            "Epoch [14/50], Loss: 0.4457, Best Threshold: 0.1944, Recall: 0.9318, Precision: 0.5899, F2 Score: 0.8350, Accuracy: 0.6420\n",
            "Epoch [15/50], Loss: 0.4158, Best Threshold: 0.1013, Recall: 0.9318, Precision: 0.5694, F2 Score: 0.8266, Accuracy: 0.6136\n",
            "Epoch [16/50], Loss: 0.3916, Best Threshold: 0.1092, Recall: 0.9432, Precision: 0.5646, F2 Score: 0.8317, Accuracy: 0.6080\n",
            "Epoch [17/50], Loss: 0.4038, Best Threshold: 0.1054, Recall: 0.9773, Precision: 0.5375, F2 Score: 0.8398, Accuracy: 0.5682\n",
            "Epoch [18/50], Loss: 0.3762, Best Threshold: 0.1497, Recall: 0.8977, Precision: 0.6583, F2 Score: 0.8369, Accuracy: 0.7159\n",
            "Epoch [19/50], Loss: 0.3738, Best Threshold: 0.1742, Recall: 0.9545, Precision: 0.5874, F2 Score: 0.8485, Accuracy: 0.6420\n",
            "Epoch [20/50], Loss: 0.3648, Best Threshold: 0.1239, Recall: 0.8864, Precision: 0.6393, F2 Score: 0.8228, Accuracy: 0.6932\n",
            "Epoch [21/50], Loss: 0.3478, Best Threshold: 0.1020, Recall: 0.9545, Precision: 0.6000, F2 Score: 0.8537, Accuracy: 0.6591\n",
            "Epoch [22/50], Loss: 0.3444, Best Threshold: 0.1005, Recall: 0.8409, Precision: 0.6667, F2 Score: 0.7991, Accuracy: 0.7102\n",
            "Epoch [23/50], Loss: 0.3201, Best Threshold: 0.1061, Recall: 0.9545, Precision: 0.5957, F2 Score: 0.8519, Accuracy: 0.6534\n",
            "Epoch [24/50], Loss: 0.3763, Best Threshold: 0.1139, Recall: 0.8977, Precision: 0.6031, F2 Score: 0.8178, Accuracy: 0.6534\n",
            "Epoch [25/50], Loss: 0.4169, Best Threshold: 0.1053, Recall: 0.9773, Precision: 0.5621, F2 Score: 0.8515, Accuracy: 0.6080\n",
            "Epoch [26/50], Loss: 0.3068, Best Threshold: 0.1116, Recall: 0.9205, Precision: 0.6639, F2 Score: 0.8544, Accuracy: 0.7273\n",
            "Epoch [27/50], Loss: 0.2908, Best Threshold: 0.1036, Recall: 0.8295, Precision: 0.6636, F2 Score: 0.7900, Accuracy: 0.7045\n",
            "Epoch [28/50], Loss: 0.2991, Best Threshold: 0.1046, Recall: 0.8977, Precision: 0.6639, F2 Score: 0.8386, Accuracy: 0.7216\n",
            "Epoch [29/50], Loss: 0.2727, Best Threshold: 0.1196, Recall: 0.8750, Precision: 0.6814, F2 Score: 0.8280, Accuracy: 0.7330\n",
            "Epoch [30/50], Loss: 0.2750, Best Threshold: 0.1227, Recall: 0.9205, Precision: 0.6136, F2 Score: 0.8368, Accuracy: 0.6705\n",
            "Epoch [31/50], Loss: 0.2484, Best Threshold: 0.1201, Recall: 0.8295, Precision: 0.6518, F2 Score: 0.7866, Accuracy: 0.6932\n",
            "Epoch [32/50], Loss: 0.2532, Best Threshold: 0.1314, Recall: 0.9318, Precision: 0.6029, F2 Score: 0.8402, Accuracy: 0.6591\n",
            "Epoch [33/50], Loss: 0.2749, Best Threshold: 0.1790, Recall: 0.9318, Precision: 0.6406, F2 Score: 0.8542, Accuracy: 0.7045\n",
            "Epoch [34/50], Loss: 0.2315, Best Threshold: 0.2790, Recall: 0.9318, Precision: 0.6667, F2 Score: 0.8632, Accuracy: 0.7330\n",
            "Epoch [35/50], Loss: 0.2564, Best Threshold: 0.1016, Recall: 0.9545, Precision: 0.6087, F2 Score: 0.8571, Accuracy: 0.6705\n",
            "Epoch [36/50], Loss: 0.2257, Best Threshold: 0.1154, Recall: 0.8977, Precision: 0.6639, F2 Score: 0.8386, Accuracy: 0.7216\n",
            "Epoch [37/50], Loss: 0.2195, Best Threshold: 0.1388, Recall: 0.8523, Precision: 0.6579, F2 Score: 0.8047, Accuracy: 0.7045\n",
            "Epoch [38/50], Loss: 0.1965, Best Threshold: 0.1083, Recall: 0.9432, Precision: 0.5887, F2 Score: 0.8418, Accuracy: 0.6420\n",
            "Epoch [39/50], Loss: 0.2117, Best Threshold: 0.1003, Recall: 0.8977, Precision: 0.6529, F2 Score: 0.8351, Accuracy: 0.7102\n",
            "Epoch [40/50], Loss: 0.1904, Best Threshold: 0.2543, Recall: 0.9545, Precision: 0.6131, F2 Score: 0.8589, Accuracy: 0.6761\n",
            "Epoch [41/50], Loss: 0.2324, Best Threshold: 0.1180, Recall: 0.9091, Precision: 0.6349, F2 Score: 0.8368, Accuracy: 0.6932\n",
            "Epoch [42/50], Loss: 0.1839, Best Threshold: 0.1510, Recall: 0.8182, Precision: 0.7200, F2 Score: 0.7965, Accuracy: 0.7500\n",
            "Epoch [43/50], Loss: 0.1718, Best Threshold: 0.1011, Recall: 0.7955, Precision: 0.7143, F2 Score: 0.7778, Accuracy: 0.7386\n",
            "Epoch [44/50], Loss: 0.1853, Best Threshold: 0.1089, Recall: 0.9432, Precision: 0.6434, F2 Score: 0.8628, Accuracy: 0.7102\n",
            "Epoch [45/50], Loss: 0.1959, Best Threshold: 0.1744, Recall: 0.8977, Precision: 0.6752, F2 Score: 0.8422, Accuracy: 0.7330\n",
            "Epoch [46/50], Loss: 0.1872, Best Threshold: 0.1065, Recall: 0.9545, Precision: 0.6222, F2 Score: 0.8624, Accuracy: 0.6875\n",
            "Epoch [47/50], Loss: 0.1833, Best Threshold: 0.1178, Recall: 0.9205, Precision: 0.6585, F2 Score: 0.8526, Accuracy: 0.7216\n",
            "Epoch [48/50], Loss: 0.1653, Best Threshold: 0.1457, Recall: 0.9205, Precision: 0.6639, F2 Score: 0.8544, Accuracy: 0.7273\n",
            "Epoch [49/50], Loss: 0.1651, Best Threshold: 0.1255, Recall: 0.9205, Precision: 0.6585, F2 Score: 0.8526, Accuracy: 0.7216\n",
            "Epoch [50/50], Loss: 0.1608, Best Threshold: 0.1190, Recall: 0.8295, Precision: 0.7300, F2 Score: 0.8075, Accuracy: 0.7614\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normal Version\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "train_dataset_db1 = CustomDataset(X_static_train_db1, X_glu_train, X_weight_train, y_train_db1)\n",
        "test_dataset_db1 = CustomDataset(X_static_test_db1, X_glu_test, X_weight_test, y_test_db1)\n",
        "\n",
        "train_loader_db1 = DataLoader(train_dataset_db1, batch_size=32, shuffle=True)\n",
        "test_loader_db1 = DataLoader(test_dataset_db1, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the Multi-Input Model\n",
        "class MultiInputModel(nn.Module):\n",
        "    def __init__(self, static_input_dim, glu_input_dim, weight_input_dim):\n",
        "        super(MultiInputModel, self).__init__()\n",
        "\n",
        "        # Static feature branch\n",
        "        self.static_fc = nn.Sequential(\n",
        "            nn.Linear(static_input_dim, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # LSTM branch for glu with attention and masking\n",
        "        self.glu_lstm = nn.LSTM(input_size=1, hidden_size=64, num_layers=2, batch_first=True)\n",
        "        self.glu_attention = nn.Sequential(\n",
        "            nn.Linear(64, 1),  # Attention score for each time step\n",
        "            nn.Softmax(dim=1)  # Normalize scores across valid time steps\n",
        "        )\n",
        "        self.glu_fc = nn.Sequential(\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # LSTM branch for weight with attention and masking\n",
        "        self.weight_lstm = nn.LSTM(input_size=1, hidden_size=64, num_layers=2, batch_first=True)\n",
        "        self.weight_attention = nn.Sequential(\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "        self.weight_fc = nn.Sequential(\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Combined high-level features\n",
        "        self.combined_fc = nn.Sequential(\n",
        "            nn.Linear(32 + 32 + 32, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, static_input, glu_input, weight_input):\n",
        "        # Static feature branch\n",
        "        static_out = self.static_fc(static_input)\n",
        "\n",
        "        # Glu branch with masking and attention\n",
        "        glu_mask = (glu_input.sum(dim=-1) != 0).float()  # Mask for valid time steps\n",
        "        glu_out, _ = self.glu_lstm(glu_input)  # Output shape: (batch_size, seq_len, hidden_size)\n",
        "\n",
        "        attention_scores_glu = self.glu_attention(glu_out).squeeze(-1)  # Attention scores: (batch_size, seq_len)\n",
        "        attention_scores_glu = attention_scores_glu * glu_mask  # Apply mask to attention scores\n",
        "        attention_weights_glu = attention_scores_glu / (attention_scores_glu.sum(dim=1, keepdim=True) + 1e-8)  # Normalize\n",
        "\n",
        "        glu_out = torch.bmm(attention_weights_glu.unsqueeze(1), glu_out).squeeze(1)  # Weighted sum: (batch_size, hidden_size)\n",
        "        glu_out = self.glu_fc(glu_out)\n",
        "\n",
        "        # Weight branch with masking and attention\n",
        "        weight_mask = (weight_input.sum(dim=-1) != 0).float()  # Mask for valid time steps\n",
        "        weight_out, _ = self.weight_lstm(weight_input)\n",
        "\n",
        "        attention_scores_weight = self.weight_attention(weight_out).squeeze(-1)  # Attention scores: (batch_size, seq_len)\n",
        "        attention_scores_weight = attention_scores_weight * weight_mask  # Apply mask to attention scores\n",
        "        attention_weights_weight = attention_scores_weight / (attention_scores_weight.sum(dim=1, keepdim=True) + 1e-8)  # Normalize\n",
        "\n",
        "        weight_out = torch.bmm(attention_weights_weight.unsqueeze(1), weight_out).squeeze(1)  # Weighted sum: (batch_size, hidden_size)\n",
        "        weight_out = self.weight_fc(weight_out)\n",
        "\n",
        "        # Combine all branches\n",
        "        combined = torch.cat([static_out, glu_out, weight_out], dim=1)\n",
        "        output = self.combined_fc(combined)\n",
        "        return output\n",
        "\n",
        "\n",
        "# weighted loss function\n",
        "num_positive = target_db1.sum()\n",
        "num_negative = len(target_db1) - num_positive\n",
        "pos_weight_value = num_negative / num_positive\n",
        "pos_weight = torch.tensor([pos_weight_value], dtype=torch.float32)\n",
        "\n",
        "# Initialize the Model, Loss, and Optimizer\n",
        "static_input_dim = X_static_train_db1.shape[1]\n",
        "glu_input_dim = X_glu_train.shape[1]\n",
        "weight_input_dim = X_weight_train.shape[1]\n",
        "\n",
        "model = MultiInputModel(static_input_dim, glu_input_dim, weight_input_dim)\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for binary classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True)\n",
        "\n",
        "# Training Model\n",
        "def train_model(model, train_loader, test_loader, criterion, optimizer, epochs=50, threshold=0.5):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for batch in train_loader:\n",
        "            static_input = batch[\"static\"]\n",
        "            glu_input = batch[\"glu\"]\n",
        "            weight_input = batch[\"weight\"]\n",
        "            target = batch[\"target\"].unsqueeze(1)  # Ensure target is of shape (batch_size, 1)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(static_input, glu_input, weight_input)\n",
        "            outputs = outputs.clamp(min=1e-7, max=1 - 1e-7)  # Clamp outputs for numerical stability\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, target)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Evaluate on the test set\n",
        "        _, _, recall, precision, f2_score, accuracy = evaluate_model(model, test_loader, threshold)\n",
        "        scheduler.step(f2_score)\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {running_loss / len(train_loader):.4f}, \"\n",
        "              f\" Recall: {recall:.4f}, Precision: {precision:.4f}, \"\n",
        "              f\"F2 Score: {f2_score:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "def calculate_metrics(outputs, targets, threshold=0.5):\n",
        "    predictions = (outputs > threshold).astype(int)\n",
        "    targets = targets.astype(int)\n",
        "    recall = recall_score(targets, predictions)\n",
        "    precision = precision_score(targets, predictions)\n",
        "    f2_score = fbeta_score(targets, predictions, beta=2)\n",
        "    accuracy = accuracy_score(targets, predictions)\n",
        "    return recall, precision, f2_score, accuracy\n",
        "\n",
        "# Evaluation\n",
        "def evaluate_model(model, test_loader, threshold=0.5):\n",
        "    model.eval()\n",
        "    all_outputs = []\n",
        "    all_targets = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            static_input = batch[\"static\"]\n",
        "            glu_input = batch[\"glu\"]\n",
        "            weight_input = batch[\"weight\"]\n",
        "            target = batch[\"target\"]\n",
        "\n",
        "            outputs = model(static_input, glu_input, weight_input)\n",
        "            all_outputs.append(outputs.squeeze().numpy())\n",
        "            all_targets.append(target.numpy())\n",
        "\n",
        "    outputs = np.concatenate(all_outputs)\n",
        "    targets = np.concatenate(all_targets)\n",
        "    recall, precision, f2_score, accuracy = calculate_metrics(outputs, targets, threshold)\n",
        "    return outputs, targets, recall, precision, f2_score, accuracy\n",
        "\n",
        "train_model(model, train_loader_db1, test_loader_db1, criterion, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrAQlJnfQROC",
        "outputId": "f1fa27e7-1bc1-4204-e2a4-63ff430ca845"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 0.6927,  Recall: 0.1705, Precision: 0.6250, F2 Score: 0.1995, Accuracy: 0.5341\n",
            "Epoch [2/50], Loss: 0.6725,  Recall: 0.1818, Precision: 0.7273, F2 Score: 0.2139, Accuracy: 0.5568\n",
            "Epoch [3/50], Loss: 0.6414,  Recall: 0.3523, Precision: 0.7949, F2 Score: 0.3964, Accuracy: 0.6307\n",
            "Epoch [4/50], Loss: 0.5986,  Recall: 0.6250, Precision: 0.6548, F2 Score: 0.6307, Accuracy: 0.6477\n",
            "Epoch [5/50], Loss: 0.5684,  Recall: 0.6932, Precision: 0.6354, F2 Score: 0.6808, Accuracy: 0.6477\n",
            "Epoch [6/50], Loss: 0.5467,  Recall: 0.4432, Precision: 0.7647, F2 Score: 0.4839, Accuracy: 0.6534\n",
            "Epoch [7/50], Loss: 0.5117,  Recall: 0.6932, Precision: 0.6630, F2 Score: 0.6869, Accuracy: 0.6705\n",
            "Epoch [8/50], Loss: 0.4970,  Recall: 0.7955, Precision: 0.6364, F2 Score: 0.7576, Accuracy: 0.6705\n",
            "Epoch [9/50], Loss: 0.4793,  Recall: 0.5795, Precision: 0.6986, F2 Score: 0.6000, Accuracy: 0.6648\n",
            "Epoch [10/50], Loss: 0.4508,  Recall: 0.6136, Precision: 0.7105, F2 Score: 0.6308, Accuracy: 0.6818\n",
            "Epoch [11/50], Loss: 0.4683,  Recall: 0.5682, Precision: 0.7353, F2 Score: 0.5952, Accuracy: 0.6818\n",
            "Epoch [12/50], Loss: 0.4276,  Recall: 0.6932, Precision: 0.7176, F2 Score: 0.6979, Accuracy: 0.7102\n",
            "Epoch [13/50], Loss: 0.3980,  Recall: 0.5455, Precision: 0.7742, F2 Score: 0.5797, Accuracy: 0.6932\n",
            "Epoch [14/50], Loss: 0.3822,  Recall: 0.6705, Precision: 0.7662, F2 Score: 0.6876, Accuracy: 0.7330\n",
            "Epoch [15/50], Loss: 0.3408,  Recall: 0.6364, Precision: 0.7568, F2 Score: 0.6573, Accuracy: 0.7159\n",
            "Epoch [16/50], Loss: 0.3199,  Recall: 0.5000, Precision: 0.8000, F2 Score: 0.5405, Accuracy: 0.6875\n",
            "Epoch [17/50], Loss: 0.3233,  Recall: 0.6250, Precision: 0.8088, F2 Score: 0.6548, Accuracy: 0.7386\n",
            "Epoch [18/50], Loss: 0.3013,  Recall: 0.5909, Precision: 0.7879, F2 Score: 0.6220, Accuracy: 0.7159\n",
            "Epoch [19/50], Loss: 0.2952,  Recall: 0.6705, Precision: 0.7108, F2 Score: 0.6782, Accuracy: 0.6989\n",
            "Epoch [20/50], Loss: 0.2854,  Recall: 0.6591, Precision: 0.7436, F2 Score: 0.6744, Accuracy: 0.7159\n",
            "Epoch [21/50], Loss: 0.2581,  Recall: 0.7045, Precision: 0.7381, F2 Score: 0.7110, Accuracy: 0.7273\n",
            "Epoch [22/50], Loss: 0.2444,  Recall: 0.6136, Precision: 0.8060, F2 Score: 0.6444, Accuracy: 0.7330\n",
            "Epoch [23/50], Loss: 0.2411,  Recall: 0.6705, Precision: 0.7195, F2 Score: 0.6797, Accuracy: 0.7045\n",
            "Epoch [24/50], Loss: 0.2357,  Recall: 0.6705, Precision: 0.7763, F2 Score: 0.6893, Accuracy: 0.7386\n",
            "Epoch [25/50], Loss: 0.2359,  Recall: 0.6818, Precision: 0.7500, F2 Score: 0.6944, Accuracy: 0.7273\n",
            "Epoch [26/50], Loss: 0.2229,  Recall: 0.6477, Precision: 0.7703, F2 Score: 0.6690, Accuracy: 0.7273\n",
            "Epoch [27/50], Loss: 0.2143,  Recall: 0.6818, Precision: 0.7407, F2 Score: 0.6928, Accuracy: 0.7216\n",
            "Epoch [28/50], Loss: 0.2072,  Recall: 0.6591, Precision: 0.7342, F2 Score: 0.6729, Accuracy: 0.7102\n",
            "Epoch [29/50], Loss: 0.2017,  Recall: 0.7159, Precision: 0.7500, F2 Score: 0.7225, Accuracy: 0.7386\n",
            "Epoch [30/50], Loss: 0.2032,  Recall: 0.6818, Precision: 0.7317, F2 Score: 0.6912, Accuracy: 0.7159\n",
            "Epoch [31/50], Loss: 0.1981,  Recall: 0.7500, Precision: 0.7253, F2 Score: 0.7449, Accuracy: 0.7330\n",
            "Epoch [32/50], Loss: 0.1982,  Recall: 0.6705, Precision: 0.7763, F2 Score: 0.6893, Accuracy: 0.7386\n",
            "Epoch [33/50], Loss: 0.1930,  Recall: 0.6818, Precision: 0.7317, F2 Score: 0.6912, Accuracy: 0.7159\n",
            "Epoch [34/50], Loss: 0.1886,  Recall: 0.6705, Precision: 0.7284, F2 Score: 0.6813, Accuracy: 0.7102\n",
            "Epoch [35/50], Loss: 0.1843,  Recall: 0.6818, Precision: 0.7229, F2 Score: 0.6897, Accuracy: 0.7102\n",
            "Epoch [36/50], Loss: 0.1833,  Recall: 0.6705, Precision: 0.7564, F2 Score: 0.6860, Accuracy: 0.7273\n",
            "Epoch [37/50], Loss: 0.1835,  Recall: 0.6477, Precision: 0.7500, F2 Score: 0.6659, Accuracy: 0.7159\n",
            "Epoch [38/50], Loss: 0.1800,  Recall: 0.7273, Precision: 0.7191, F2 Score: 0.7256, Accuracy: 0.7216\n",
            "Epoch [39/50], Loss: 0.1788,  Recall: 0.6818, Precision: 0.7317, F2 Score: 0.6912, Accuracy: 0.7159\n",
            "Epoch [40/50], Loss: 0.1766,  Recall: 0.6818, Precision: 0.7317, F2 Score: 0.6912, Accuracy: 0.7159\n",
            "Epoch [41/50], Loss: 0.1753,  Recall: 0.6818, Precision: 0.7317, F2 Score: 0.6912, Accuracy: 0.7159\n",
            "Epoch [42/50], Loss: 0.1752,  Recall: 0.6818, Precision: 0.7317, F2 Score: 0.6912, Accuracy: 0.7159\n",
            "Epoch [43/50], Loss: 0.1742,  Recall: 0.6818, Precision: 0.7317, F2 Score: 0.6912, Accuracy: 0.7159\n",
            "Epoch [44/50], Loss: 0.1737,  Recall: 0.7045, Precision: 0.7209, F2 Score: 0.7078, Accuracy: 0.7159\n",
            "Epoch [45/50], Loss: 0.1726,  Recall: 0.6818, Precision: 0.7407, F2 Score: 0.6928, Accuracy: 0.7216\n",
            "Epoch [46/50], Loss: 0.1751,  Recall: 0.6818, Precision: 0.7317, F2 Score: 0.6912, Accuracy: 0.7159\n",
            "Epoch [47/50], Loss: 0.1724,  Recall: 0.6818, Precision: 0.7407, F2 Score: 0.6928, Accuracy: 0.7216\n",
            "Epoch [48/50], Loss: 0.1708,  Recall: 0.6932, Precision: 0.7262, F2 Score: 0.6995, Accuracy: 0.7159\n",
            "Epoch [49/50], Loss: 0.1711,  Recall: 0.6932, Precision: 0.7349, F2 Score: 0.7011, Accuracy: 0.7216\n",
            "Epoch [50/50], Loss: 0.1703,  Recall: 0.6818, Precision: 0.7500, F2 Score: 0.6944, Accuracy: 0.7273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# weighted loss function + focus loss\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "train_dataset_db1 = CustomDataset(X_static_train_db1, X_glu_train, X_weight_train, y_train_db1)\n",
        "test_dataset_db1 = CustomDataset(X_static_test_db1, X_glu_test, X_weight_test, y_test_db1)\n",
        "\n",
        "train_loader_db1 = DataLoader(train_dataset_db1, batch_size=32, shuffle=True)\n",
        "test_loader_db1 = DataLoader(test_dataset_db1, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the Multi-Input Model\n",
        "class MultiInputModel(nn.Module):\n",
        "    def __init__(self, static_input_dim, glu_input_dim, weight_input_dim):\n",
        "        super(MultiInputModel, self).__init__()\n",
        "\n",
        "        # Static feature branch\n",
        "        self.static_fc = nn.Sequential(\n",
        "            nn.Linear(static_input_dim, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # LSTM branch for glu with attention and masking\n",
        "        self.glu_lstm = nn.LSTM(input_size=1, hidden_size=64, num_layers=2, batch_first=True)\n",
        "        self.glu_attention = nn.Sequential(\n",
        "            nn.Linear(64, 1),  # Attention score for each time step\n",
        "            nn.Softmax(dim=1)  # Normalize scores across valid time steps\n",
        "        )\n",
        "        self.glu_fc = nn.Sequential(\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # LSTM branch for weight with attention and masking\n",
        "        self.weight_lstm = nn.LSTM(input_size=1, hidden_size=64, num_layers=2, batch_first=True)\n",
        "        self.weight_attention = nn.Sequential(\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "        self.weight_fc = nn.Sequential(\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Combined high-level features\n",
        "        self.combined_fc = nn.Sequential(\n",
        "            nn.Linear(32 + 32 + 32, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, static_input, glu_input, weight_input):\n",
        "        # Static feature branch\n",
        "        static_out = self.static_fc(static_input)\n",
        "\n",
        "        # Glu branch with masking and attention\n",
        "        glu_mask = (glu_input.sum(dim=-1) != 0).float()  # Mask for valid time steps\n",
        "        glu_out, _ = self.glu_lstm(glu_input)  # Output shape: (batch_size, seq_len, hidden_size)\n",
        "\n",
        "        attention_scores_glu = self.glu_attention(glu_out).squeeze(-1)  # Attention scores: (batch_size, seq_len)\n",
        "        attention_scores_glu = attention_scores_glu * glu_mask  # Apply mask to attention scores\n",
        "        attention_weights_glu = attention_scores_glu / (attention_scores_glu.sum(dim=1, keepdim=True) + 1e-8)  # Normalize\n",
        "\n",
        "        glu_out = torch.bmm(attention_weights_glu.unsqueeze(1), glu_out).squeeze(1)  # Weighted sum: (batch_size, hidden_size)\n",
        "        glu_out = self.glu_fc(glu_out)\n",
        "\n",
        "        # Weight branch with masking and attention\n",
        "        weight_mask = (weight_input.sum(dim=-1) != 0).float()  # Mask for valid time steps\n",
        "        weight_out, _ = self.weight_lstm(weight_input)\n",
        "\n",
        "        attention_scores_weight = self.weight_attention(weight_out).squeeze(-1)  # Attention scores: (batch_size, seq_len)\n",
        "        attention_scores_weight = attention_scores_weight * weight_mask  # Apply mask to attention scores\n",
        "        attention_weights_weight = attention_scores_weight / (attention_scores_weight.sum(dim=1, keepdim=True) + 1e-8)  # Normalize\n",
        "\n",
        "        weight_out = torch.bmm(attention_weights_weight.unsqueeze(1), weight_out).squeeze(1)  # Weighted sum: (batch_size, hidden_size)\n",
        "        weight_out = self.weight_fc(weight_out)\n",
        "\n",
        "        # Combine all branches\n",
        "        combined = torch.cat([static_out, glu_out, weight_out], dim=1)\n",
        "        output = self.combined_fc(combined)\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "class WeightedFocalLoss(nn.Module):\n",
        "    def __init__(self, pos_weight, gamma=2):\n",
        "        super(WeightedFocalLoss, self).__init__()\n",
        "        self.pos_weight = pos_weight\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        BCE_loss = nn.BCELoss(weight=(targets * self.pos_weight + (1 - targets)))(inputs, targets)\n",
        "        pt = torch.exp(-BCE_loss)  # pt is the probability of the correct class\n",
        "        focal_loss = (1 - pt) ** self.gamma * BCE_loss\n",
        "        return torch.mean(focal_loss)\n",
        "\n",
        "# Initialize the Model, Loss, and Optimizer\n",
        "static_input_dim = X_static_train_db1.shape[1]\n",
        "glu_input_dim = X_glu_train.shape[1]\n",
        "weight_input_dim = X_weight_train.shape[1]\n",
        "\n",
        "model = MultiInputModel(static_input_dim, glu_input_dim, weight_input_dim)\n",
        "num_positive = target_db1.sum()\n",
        "num_negative = len(target_db1) - num_positive\n",
        "pos_weight = num_negative / num_positive\n",
        "criterion = WeightedFocalLoss(pos_weight=pos_weight, gamma=2)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True)\n",
        "\n",
        "# Training Model\n",
        "def train_model(model, train_loader, test_loader, criterion, optimizer, epochs=50, threshold=0.5):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for batch in train_loader:\n",
        "            static_input = batch[\"static\"]\n",
        "            glu_input = batch[\"glu\"]\n",
        "            weight_input = batch[\"weight\"]\n",
        "            target = batch[\"target\"].unsqueeze(1)  # Ensure target is of shape (batch_size, 1)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(static_input, glu_input, weight_input)\n",
        "            outputs = outputs.clamp(min=1e-7, max=1 - 1e-7)  # Clamp outputs for numerical stability\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, target)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Evaluate on the test set\n",
        "        _, _, recall, precision, f2_score, accuracy = evaluate_model(model, test_loader, threshold)\n",
        "        scheduler.step(f2_score)\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {running_loss / len(train_loader):.4f}, \"\n",
        "              f\" Recall: {recall:.4f}, Precision: {precision:.4f}, \"\n",
        "              f\"F2 Score: {f2_score:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "def calculate_metrics(outputs, targets, threshold=0.5):\n",
        "    predictions = (outputs > threshold).astype(int)\n",
        "    targets = targets.astype(int)\n",
        "    recall = recall_score(targets, predictions)\n",
        "    precision = precision_score(targets, predictions)\n",
        "    f2_score = fbeta_score(targets, predictions, beta=2)\n",
        "    accuracy = accuracy_score(targets, predictions)\n",
        "    return recall, precision, f2_score, accuracy\n",
        "\n",
        "# Evaluation\n",
        "def evaluate_model(model, test_loader, threshold=0.5):\n",
        "    model.eval()\n",
        "    all_outputs = []\n",
        "    all_targets = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            static_input = batch[\"static\"]\n",
        "            glu_input = batch[\"glu\"]\n",
        "            weight_input = batch[\"weight\"]\n",
        "            target = batch[\"target\"]\n",
        "\n",
        "            outputs = model(static_input, glu_input, weight_input)\n",
        "            all_outputs.append(outputs.squeeze().numpy())\n",
        "            all_targets.append(target.numpy())\n",
        "\n",
        "    outputs = np.concatenate(all_outputs)\n",
        "    targets = np.concatenate(all_targets)\n",
        "    recall, precision, f2_score, accuracy = calculate_metrics(outputs, targets, threshold)\n",
        "    return outputs, targets, recall, precision, f2_score, accuracy\n",
        "\n",
        "train_model(model, train_loader_db1, test_loader_db1, criterion, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LTRJMfHQYtR",
        "outputId": "8ad59482-1bd9-46e1-c89c-17973f688dea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 0.5550,  Recall: 1.0000, Precision: 0.5000, F2 Score: 0.8333, Accuracy: 0.5000\n",
            "Epoch [2/50], Loss: 0.4552,  Recall: 1.0000, Precision: 0.5000, F2 Score: 0.8333, Accuracy: 0.5000\n",
            "Epoch [3/50], Loss: 0.4141,  Recall: 1.0000, Precision: 0.5000, F2 Score: 0.8333, Accuracy: 0.5000\n",
            "Epoch [4/50], Loss: 0.3693,  Recall: 1.0000, Precision: 0.5000, F2 Score: 0.8333, Accuracy: 0.5000\n",
            "Epoch [5/50], Loss: 0.3399,  Recall: 0.9091, Precision: 0.5882, F2 Score: 0.8197, Accuracy: 0.6364\n",
            "Epoch [6/50], Loss: 0.3108,  Recall: 0.7273, Precision: 0.6337, F2 Score: 0.7064, Accuracy: 0.6534\n",
            "Epoch [7/50], Loss: 0.2909,  Recall: 0.6932, Precision: 0.7011, F2 Score: 0.6948, Accuracy: 0.6989\n",
            "Epoch [8/50], Loss: 0.2600,  Recall: 0.8409, Precision: 0.6016, F2 Score: 0.7789, Accuracy: 0.6420\n",
            "Epoch [9/50], Loss: 0.2391,  Recall: 0.7159, Precision: 0.6562, F2 Score: 0.7031, Accuracy: 0.6705\n",
            "Epoch [10/50], Loss: 0.2202,  Recall: 0.8977, Precision: 0.6077, F2 Score: 0.8195, Accuracy: 0.6591\n",
            "Epoch [11/50], Loss: 0.2113,  Recall: 0.7955, Precision: 0.6422, F2 Score: 0.7592, Accuracy: 0.6761\n",
            "Epoch [12/50], Loss: 0.1913,  Recall: 0.7500, Precision: 0.6408, F2 Score: 0.7253, Accuracy: 0.6648\n",
            "Epoch [13/50], Loss: 0.1854,  Recall: 0.8750, Precision: 0.6160, F2 Score: 0.8071, Accuracy: 0.6648\n",
            "Epoch [14/50], Loss: 0.1557,  Recall: 0.7955, Precision: 0.6481, F2 Score: 0.7609, Accuracy: 0.6818\n",
            "Epoch [15/50], Loss: 0.1540,  Recall: 0.8295, Precision: 0.6293, F2 Score: 0.7799, Accuracy: 0.6705\n",
            "Epoch [16/50], Loss: 0.1424,  Recall: 0.8068, Precision: 0.6396, F2 Score: 0.7667, Accuracy: 0.6761\n",
            "Epoch [17/50], Loss: 0.1515,  Recall: 0.7727, Precision: 0.6800, F2 Score: 0.7522, Accuracy: 0.7045\n",
            "Epoch [18/50], Loss: 0.1359,  Recall: 0.7841, Precision: 0.6389, F2 Score: 0.7500, Accuracy: 0.6705\n",
            "Epoch [19/50], Loss: 0.1327,  Recall: 0.7273, Precision: 0.6531, F2 Score: 0.7111, Accuracy: 0.6705\n",
            "Epoch [20/50], Loss: 0.1196,  Recall: 0.7955, Precision: 0.6481, F2 Score: 0.7609, Accuracy: 0.6818\n",
            "Epoch [21/50], Loss: 0.1167,  Recall: 0.8068, Precision: 0.6514, F2 Score: 0.7701, Accuracy: 0.6875\n",
            "Epoch [22/50], Loss: 0.1263,  Recall: 0.7955, Precision: 0.6422, F2 Score: 0.7592, Accuracy: 0.6761\n",
            "Epoch [23/50], Loss: 0.1237,  Recall: 0.8182, Precision: 0.6486, F2 Score: 0.7775, Accuracy: 0.6875\n",
            "Epoch [24/50], Loss: 0.1137,  Recall: 0.7500, Precision: 0.6804, F2 Score: 0.7350, Accuracy: 0.6989\n",
            "Epoch [25/50], Loss: 0.1095,  Recall: 0.8182, Precision: 0.6429, F2 Score: 0.7759, Accuracy: 0.6818\n",
            "Epoch [26/50], Loss: 0.1056,  Recall: 0.7614, Precision: 0.6634, F2 Score: 0.7395, Accuracy: 0.6875\n",
            "Epoch [27/50], Loss: 0.1071,  Recall: 0.7841, Precision: 0.6571, F2 Score: 0.7549, Accuracy: 0.6875\n",
            "Epoch [28/50], Loss: 0.1018,  Recall: 0.8295, Precision: 0.6460, F2 Score: 0.7849, Accuracy: 0.6875\n",
            "Epoch [29/50], Loss: 0.1075,  Recall: 0.8182, Precision: 0.6486, F2 Score: 0.7775, Accuracy: 0.6875\n",
            "Epoch [30/50], Loss: 0.0989,  Recall: 0.7841, Precision: 0.6571, F2 Score: 0.7549, Accuracy: 0.6875\n",
            "Epoch [31/50], Loss: 0.0999,  Recall: 0.7955, Precision: 0.6604, F2 Score: 0.7642, Accuracy: 0.6932\n",
            "Epoch [32/50], Loss: 0.1005,  Recall: 0.7841, Precision: 0.6571, F2 Score: 0.7549, Accuracy: 0.6875\n",
            "Epoch [33/50], Loss: 0.1005,  Recall: 0.7727, Precision: 0.6602, F2 Score: 0.7473, Accuracy: 0.6875\n",
            "Epoch [34/50], Loss: 0.0973,  Recall: 0.7727, Precision: 0.6667, F2 Score: 0.7489, Accuracy: 0.6932\n",
            "Epoch [35/50], Loss: 0.0967,  Recall: 0.7614, Precision: 0.6569, F2 Score: 0.7379, Accuracy: 0.6818\n",
            "Epoch [36/50], Loss: 0.0965,  Recall: 0.7614, Precision: 0.6569, F2 Score: 0.7379, Accuracy: 0.6818\n",
            "Epoch [37/50], Loss: 0.0978,  Recall: 0.7614, Precision: 0.6505, F2 Score: 0.7363, Accuracy: 0.6761\n",
            "Epoch [38/50], Loss: 0.0931,  Recall: 0.7614, Precision: 0.6505, F2 Score: 0.7363, Accuracy: 0.6761\n",
            "Epoch [39/50], Loss: 0.0938,  Recall: 0.7614, Precision: 0.6505, F2 Score: 0.7363, Accuracy: 0.6761\n",
            "Epoch [40/50], Loss: 0.0962,  Recall: 0.7614, Precision: 0.6505, F2 Score: 0.7363, Accuracy: 0.6761\n",
            "Epoch [41/50], Loss: 0.0915,  Recall: 0.7614, Precision: 0.6505, F2 Score: 0.7363, Accuracy: 0.6761\n",
            "Epoch [42/50], Loss: 0.1002,  Recall: 0.7614, Precision: 0.6505, F2 Score: 0.7363, Accuracy: 0.6761\n",
            "Epoch [43/50], Loss: 0.0953,  Recall: 0.7727, Precision: 0.6538, F2 Score: 0.7456, Accuracy: 0.6818\n",
            "Epoch [44/50], Loss: 0.0964,  Recall: 0.7614, Precision: 0.6569, F2 Score: 0.7379, Accuracy: 0.6818\n",
            "Epoch [45/50], Loss: 0.0983,  Recall: 0.7614, Precision: 0.6569, F2 Score: 0.7379, Accuracy: 0.6818\n",
            "Epoch [46/50], Loss: 0.0898,  Recall: 0.7614, Precision: 0.6569, F2 Score: 0.7379, Accuracy: 0.6818\n",
            "Epoch [47/50], Loss: 0.0934,  Recall: 0.7614, Precision: 0.6505, F2 Score: 0.7363, Accuracy: 0.6761\n",
            "Epoch [48/50], Loss: 0.0918,  Recall: 0.7727, Precision: 0.6538, F2 Score: 0.7456, Accuracy: 0.6818\n",
            "Epoch [49/50], Loss: 0.0910,  Recall: 0.7614, Precision: 0.6569, F2 Score: 0.7379, Accuracy: 0.6818\n",
            "Epoch [50/50], Loss: 0.0969,  Recall: 0.7614, Precision: 0.6569, F2 Score: 0.7379, Accuracy: 0.6818\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Without scheduler\n",
        "train_dataset_db1 = CustomDataset(X_static_train_db1, X_glu_train, X_weight_train, y_train_db1)\n",
        "test_dataset_db1 = CustomDataset(X_static_test_db1, X_glu_test, X_weight_test, y_test_db1)\n",
        "\n",
        "train_loader_db1 = DataLoader(train_dataset_db1, batch_size=32, shuffle=True)\n",
        "test_loader_db1 = DataLoader(test_dataset_db1, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the Multi-Input Model\n",
        "class MultiInputModel(nn.Module):\n",
        "    def __init__(self, static_input_dim, glu_input_dim, weight_input_dim):\n",
        "        super(MultiInputModel, self).__init__()\n",
        "\n",
        "        # Static feature branch\n",
        "        self.static_fc = nn.Sequential(\n",
        "            nn.Linear(static_input_dim, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # LSTM branch for glu with attention and masking\n",
        "        self.glu_lstm = nn.LSTM(input_size=1, hidden_size=64, num_layers=2, batch_first=True)\n",
        "        self.glu_attention = nn.Sequential(\n",
        "            nn.Linear(64, 1),  # Attention score for each time step\n",
        "            nn.Softmax(dim=1)  # Normalize scores across valid time steps\n",
        "        )\n",
        "        self.glu_fc = nn.Sequential(\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # LSTM branch for weight with attention and masking\n",
        "        self.weight_lstm = nn.LSTM(input_size=1, hidden_size=64, num_layers=2, batch_first=True)\n",
        "        self.weight_attention = nn.Sequential(\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "        self.weight_fc = nn.Sequential(\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Combined high-level features\n",
        "        self.combined_fc = nn.Sequential(\n",
        "            nn.Linear(32 + 32 + 32, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, static_input, glu_input, weight_input):\n",
        "        # Static feature branch\n",
        "        static_out = self.static_fc(static_input)\n",
        "\n",
        "        # Glu branch with masking and attention\n",
        "        glu_mask = (glu_input.sum(dim=-1) != 0).float()  # Mask for valid time steps\n",
        "        glu_out, _ = self.glu_lstm(glu_input)  # Output shape: (batch_size, seq_len, hidden_size)\n",
        "\n",
        "        attention_scores_glu = self.glu_attention(glu_out).squeeze(-1)  # Attention scores: (batch_size, seq_len)\n",
        "        attention_scores_glu = attention_scores_glu * glu_mask  # Apply mask to attention scores\n",
        "        attention_weights_glu = attention_scores_glu / (attention_scores_glu.sum(dim=1, keepdim=True) + 1e-8)  # Normalize\n",
        "\n",
        "        glu_out = torch.bmm(attention_weights_glu.unsqueeze(1), glu_out).squeeze(1)  # Weighted sum: (batch_size, hidden_size)\n",
        "        glu_out = self.glu_fc(glu_out)\n",
        "\n",
        "        # Weight branch with masking and attention\n",
        "        weight_mask = (weight_input.sum(dim=-1) != 0).float()  # Mask for valid time steps\n",
        "        weight_out, _ = self.weight_lstm(weight_input)\n",
        "\n",
        "        attention_scores_weight = self.weight_attention(weight_out).squeeze(-1)  # Attention scores: (batch_size, seq_len)\n",
        "        attention_scores_weight = attention_scores_weight * weight_mask  # Apply mask to attention scores\n",
        "        attention_weights_weight = attention_scores_weight / (attention_scores_weight.sum(dim=1, keepdim=True) + 1e-8)  # Normalize\n",
        "\n",
        "        weight_out = torch.bmm(attention_weights_weight.unsqueeze(1), weight_out).squeeze(1)  # Weighted sum: (batch_size, hidden_size)\n",
        "        weight_out = self.weight_fc(weight_out)\n",
        "\n",
        "        # Combine all branches\n",
        "        combined = torch.cat([static_out, glu_out, weight_out], dim=1)\n",
        "        output = self.combined_fc(combined)\n",
        "        return output\n",
        "\n",
        "\n",
        "# weighted loss function\n",
        "num_positive = target_db1.sum()\n",
        "num_negative = len(target_db1) - num_positive\n",
        "pos_weight_value = num_negative / num_positive\n",
        "pos_weight = torch.tensor([pos_weight_value], dtype=torch.float32)\n",
        "\n",
        "# Initialize the Model, Loss, and Optimizer\n",
        "static_input_dim = X_static_train_db1.shape[1]\n",
        "glu_input_dim = X_glu_train.shape[1]\n",
        "weight_input_dim = X_weight_train.shape[1]\n",
        "\n",
        "model = MultiInputModel(static_input_dim, glu_input_dim, weight_input_dim)\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for binary classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "\n",
        "# Training Model\n",
        "def train_model(model, train_loader, test_loader, criterion, optimizer, epochs=50, threshold=0.5):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for batch in train_loader:\n",
        "            static_input = batch[\"static\"]\n",
        "            glu_input = batch[\"glu\"]\n",
        "            weight_input = batch[\"weight\"]\n",
        "            target = batch[\"target\"].unsqueeze(1)\n",
        "            outputs = model(static_input, glu_input, weight_input)\n",
        "            loss = criterion(outputs, target)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        _, _, recall, precision, f2_score, accuracy = evaluate_model(model, test_loader, threshold)\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {running_loss / len(train_loader):.4f}, \"\n",
        "              f\" Recall: {recall:.4f}, Precision: {precision:.4f}, \"\n",
        "              f\"F2 Score: {f2_score:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "def calculate_metrics(outputs, targets, threshold=0.5):\n",
        "    predictions = (outputs > threshold).astype(int)\n",
        "    targets = targets.astype(int)\n",
        "    recall = recall_score(targets, predictions)\n",
        "    precision = precision_score(targets, predictions)\n",
        "    f2_score = fbeta_score(targets, predictions, beta=2)\n",
        "    accuracy = accuracy_score(targets, predictions)\n",
        "    return recall, precision, f2_score, accuracy\n",
        "\n",
        "# Evaluation\n",
        "def evaluate_model(model, test_loader, threshold=0.5):\n",
        "    model.eval()\n",
        "    all_outputs = []\n",
        "    all_targets = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            static_input = batch[\"static\"]\n",
        "            glu_input = batch[\"glu\"]\n",
        "            weight_input = batch[\"weight\"]\n",
        "            target = batch[\"target\"]\n",
        "\n",
        "            outputs = model(static_input, glu_input, weight_input)\n",
        "            all_outputs.append(outputs.squeeze().numpy())\n",
        "            all_targets.append(target.numpy())\n",
        "\n",
        "    outputs = np.concatenate(all_outputs)\n",
        "    targets = np.concatenate(all_targets)\n",
        "    recall, precision, f2_score, accuracy = calculate_metrics(outputs, targets, threshold)\n",
        "    return outputs, targets, recall, precision, f2_score, accuracy\n",
        "\n",
        "train_model(model, train_loader_db1, test_loader_db1, criterion, optimizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csXqiYvsSg9W",
        "outputId": "15d5bb63-f12f-4ed2-a9bc-40d91da3aa58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 0.6933,  Recall: 0.9432, Precision: 0.5390, F2 Score: 0.8202, Accuracy: 0.5682\n",
            "Epoch [2/50], Loss: 0.6803,  Recall: 0.8068, Precision: 0.5966, F2 Score: 0.7537, Accuracy: 0.6307\n",
            "Epoch [3/50], Loss: 0.6494,  Recall: 0.3523, Precision: 0.7750, F2 Score: 0.3954, Accuracy: 0.6250\n",
            "Epoch [4/50], Loss: 0.6147,  Recall: 0.4432, Precision: 0.6610, F2 Score: 0.4745, Accuracy: 0.6080\n",
            "Epoch [5/50], Loss: 0.5810,  Recall: 0.6932, Precision: 0.5922, F2 Score: 0.6703, Accuracy: 0.6080\n",
            "Epoch [6/50], Loss: 0.5637,  Recall: 0.7159, Precision: 0.6300, F2 Score: 0.6969, Accuracy: 0.6477\n",
            "Epoch [7/50], Loss: 0.5500,  Recall: 0.5568, Precision: 0.7313, F2 Score: 0.5847, Accuracy: 0.6761\n",
            "Epoch [8/50], Loss: 0.5285,  Recall: 0.3750, Precision: 0.6735, F2 Score: 0.4115, Accuracy: 0.5966\n",
            "Epoch [9/50], Loss: 0.5165,  Recall: 0.6023, Precision: 0.6709, F2 Score: 0.6148, Accuracy: 0.6534\n",
            "Epoch [10/50], Loss: 0.5024,  Recall: 0.7045, Precision: 0.6739, F2 Score: 0.6982, Accuracy: 0.6818\n",
            "Epoch [11/50], Loss: 0.4891,  Recall: 0.4773, Precision: 0.7500, F2 Score: 0.5147, Accuracy: 0.6591\n",
            "Epoch [12/50], Loss: 0.4632,  Recall: 0.4659, Precision: 0.7736, F2 Score: 0.5062, Accuracy: 0.6648\n",
            "Epoch [13/50], Loss: 0.4447,  Recall: 0.5227, Precision: 0.7541, F2 Score: 0.5569, Accuracy: 0.6761\n",
            "Epoch [14/50], Loss: 0.4372,  Recall: 0.5000, Precision: 0.7097, F2 Score: 0.5314, Accuracy: 0.6477\n",
            "Epoch [15/50], Loss: 0.4290,  Recall: 0.4773, Precision: 0.8077, F2 Score: 0.5198, Accuracy: 0.6818\n",
            "Epoch [16/50], Loss: 0.4044,  Recall: 0.6023, Precision: 0.7067, F2 Score: 0.6206, Accuracy: 0.6761\n",
            "Epoch [17/50], Loss: 0.3825,  Recall: 0.5000, Precision: 0.7719, F2 Score: 0.5379, Accuracy: 0.6761\n",
            "Epoch [18/50], Loss: 0.3750,  Recall: 0.5455, Precision: 0.7500, F2 Score: 0.5769, Accuracy: 0.6818\n",
            "Epoch [19/50], Loss: 0.3501,  Recall: 0.5682, Precision: 0.7463, F2 Score: 0.5967, Accuracy: 0.6875\n",
            "Epoch [20/50], Loss: 0.3572,  Recall: 0.5682, Precision: 0.8065, F2 Score: 0.6039, Accuracy: 0.7159\n",
            "Epoch [21/50], Loss: 0.3350,  Recall: 0.5000, Precision: 0.7333, F2 Score: 0.5340, Accuracy: 0.6591\n",
            "Epoch [22/50], Loss: 0.3301,  Recall: 0.6705, Precision: 0.7108, F2 Score: 0.6782, Accuracy: 0.6989\n",
            "Epoch [23/50], Loss: 0.3106,  Recall: 0.5795, Precision: 0.8095, F2 Score: 0.6145, Accuracy: 0.7216\n",
            "Epoch [24/50], Loss: 0.2975,  Recall: 0.3750, Precision: 0.8462, F2 Score: 0.4220, Accuracy: 0.6534\n",
            "Epoch [25/50], Loss: 0.3436,  Recall: 0.5568, Precision: 0.8167, F2 Score: 0.5947, Accuracy: 0.7159\n",
            "Epoch [26/50], Loss: 0.2827,  Recall: 0.5000, Precision: 0.8627, F2 Score: 0.5459, Accuracy: 0.7102\n",
            "Epoch [27/50], Loss: 0.2863,  Recall: 0.6250, Precision: 0.7051, F2 Score: 0.6395, Accuracy: 0.6818\n",
            "Epoch [28/50], Loss: 0.2870,  Recall: 0.6705, Precision: 0.7468, F2 Score: 0.6845, Accuracy: 0.7216\n",
            "Epoch [29/50], Loss: 0.2535,  Recall: 0.6136, Precision: 0.7606, F2 Score: 0.6383, Accuracy: 0.7102\n",
            "Epoch [30/50], Loss: 0.2264,  Recall: 0.6705, Precision: 0.7195, F2 Score: 0.6797, Accuracy: 0.7045\n",
            "Epoch [31/50], Loss: 0.2565,  Recall: 0.5682, Precision: 0.8333, F2 Score: 0.6068, Accuracy: 0.7273\n",
            "Epoch [32/50], Loss: 0.2515,  Recall: 0.7045, Precision: 0.7381, F2 Score: 0.7110, Accuracy: 0.7273\n",
            "Epoch [33/50], Loss: 0.2397,  Recall: 0.6477, Precision: 0.7917, F2 Score: 0.6722, Accuracy: 0.7386\n",
            "Epoch [34/50], Loss: 0.2159,  Recall: 0.6818, Precision: 0.7595, F2 Score: 0.6961, Accuracy: 0.7330\n",
            "Epoch [35/50], Loss: 0.2149,  Recall: 0.5568, Precision: 0.7656, F2 Score: 0.5889, Accuracy: 0.6932\n",
            "Epoch [36/50], Loss: 0.2016,  Recall: 0.5682, Precision: 0.8197, F2 Score: 0.6053, Accuracy: 0.7216\n",
            "Epoch [37/50], Loss: 0.2064,  Recall: 0.6591, Precision: 0.7436, F2 Score: 0.6744, Accuracy: 0.7159\n",
            "Epoch [38/50], Loss: 0.2426,  Recall: 0.5909, Precision: 0.7647, F2 Score: 0.6190, Accuracy: 0.7045\n",
            "Epoch [39/50], Loss: 0.1819,  Recall: 0.6250, Precision: 0.7971, F2 Score: 0.6532, Accuracy: 0.7330\n",
            "Epoch [40/50], Loss: 0.1789,  Recall: 0.7955, Precision: 0.7447, F2 Score: 0.7848, Accuracy: 0.7614\n",
            "Epoch [41/50], Loss: 0.1742,  Recall: 0.6818, Precision: 0.7595, F2 Score: 0.6961, Accuracy: 0.7330\n",
            "Epoch [42/50], Loss: 0.1682,  Recall: 0.6477, Precision: 0.8028, F2 Score: 0.6738, Accuracy: 0.7443\n",
            "Epoch [43/50], Loss: 0.1617,  Recall: 0.7159, Precision: 0.7412, F2 Score: 0.7208, Accuracy: 0.7330\n",
            "Epoch [44/50], Loss: 0.1599,  Recall: 0.6705, Precision: 0.7662, F2 Score: 0.6876, Accuracy: 0.7330\n",
            "Epoch [45/50], Loss: 0.2030,  Recall: 0.7045, Precision: 0.7045, F2 Score: 0.7045, Accuracy: 0.7045\n",
            "Epoch [46/50], Loss: 0.1933,  Recall: 0.7500, Precision: 0.8049, F2 Score: 0.7604, Accuracy: 0.7841\n",
            "Epoch [47/50], Loss: 0.1434,  Recall: 0.6136, Precision: 0.8182, F2 Score: 0.6459, Accuracy: 0.7386\n",
            "Epoch [48/50], Loss: 0.1546,  Recall: 0.6250, Precision: 0.7971, F2 Score: 0.6532, Accuracy: 0.7330\n",
            "Epoch [49/50], Loss: 0.1368,  Recall: 0.6477, Precision: 0.7703, F2 Score: 0.6690, Accuracy: 0.7273\n",
            "Epoch [50/50], Loss: 0.1359,  Recall: 0.6477, Precision: 0.7917, F2 Score: 0.6722, Accuracy: 0.7386\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the model training for Dataset 2"
      ],
      "metadata": {
        "id": "KVydVlsBxQd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# weighted focus loss\n",
        "from sklearn.metrics import recall_score, precision_score, fbeta_score, accuracy_score\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "\n",
        "# Dataset Class\n",
        "class StaticDataset(Dataset):\n",
        "    def __init__(self, static_data, target):\n",
        "        if isinstance(static_data, pd.DataFrame):\n",
        "            self.static_data = torch.tensor(static_data.to_numpy(), dtype=torch.float32)\n",
        "        else:\n",
        "            self.static_data = torch.tensor(static_data, dtype=torch.float32)\n",
        "        self.target = torch.tensor(target, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.target)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"static\": self.static_data[idx],\n",
        "            \"target\": self.target[idx],\n",
        "        }\n",
        "\n",
        "\n",
        "# Weighted Focal Loss\n",
        "class WeightedFocalLoss(nn.Module):\n",
        "    def __init__(self, pos_weight, gamma=1.5):\n",
        "        super(WeightedFocalLoss, self).__init__()\n",
        "        self.pos_weight = pos_weight\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        BCE_loss = nn.BCELoss(weight=(targets * self.pos_weight + (1 - targets)))(inputs, targets)\n",
        "        pt = torch.exp(-BCE_loss)  # Probability of the correct class\n",
        "        focal_loss = (1 - pt) ** self.gamma * BCE_loss\n",
        "        return torch.mean(focal_loss)\n",
        "\n",
        "\n",
        "# Static Model\n",
        "class StaticModel(nn.Module):\n",
        "    def __init__(self, static_input_dim):\n",
        "        super(StaticModel, self).__init__()\n",
        "        self.static_fc = nn.Sequential(\n",
        "            nn.Linear(static_input_dim, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, static_input):\n",
        "        return self.static_fc(static_input)\n",
        "\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate_model(model, test_loader, threshold=0.5):\n",
        "    model.eval()\n",
        "    all_outputs = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            static_input = batch[\"static\"]\n",
        "            target = batch[\"target\"]\n",
        "\n",
        "            outputs = model(static_input)\n",
        "            all_outputs.append(outputs.squeeze().numpy())\n",
        "            all_targets.append(target.numpy())\n",
        "\n",
        "    outputs = np.concatenate(all_outputs)\n",
        "    targets = np.concatenate(all_targets)\n",
        "\n",
        "    # Metrics calculation\n",
        "    predictions = (outputs > threshold).astype(int)\n",
        "    recall = recall_score(targets, predictions)\n",
        "    precision = precision_score(targets, predictions)\n",
        "    f2_score = fbeta_score(targets, predictions, beta=2)\n",
        "    accuracy = accuracy_score(targets, predictions)\n",
        "\n",
        "    return outputs, targets, recall, precision, f2_score, accuracy\n",
        "\n",
        "\n",
        "# Training Function\n",
        "def train_model(model, train_loader, test_loader, criterion, optimizer, scheduler, epochs=50, threshold=0.5):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for batch in train_loader:\n",
        "            static_input = batch[\"static\"]\n",
        "            target = batch[\"target\"].unsqueeze(1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(static_input).clamp(min=1e-7, max=1 - 1e-7)\n",
        "            loss = criterion(outputs, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Evaluate on the test set\n",
        "        _, _, recall, precision, f2_score, accuracy = evaluate_model(model, test_loader, threshold)\n",
        "        scheduler.step(f2_score)\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {running_loss / len(train_loader):.4f}, \"\n",
        "              f\"Recall: {recall:.4f}, Precision: {precision:.4f}, \"\n",
        "              f\"F2 Score: {f2_score:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "# Initialize DataLoaders and Model\n",
        "train_dataset_db2 = StaticDataset(X_static_train_db2, y_train_db2)\n",
        "test_dataset_db2 = StaticDataset(X_static_test_db2, y_test_db2)\n",
        "\n",
        "train_loader_db2 = DataLoader(train_dataset_db2, batch_size=32, shuffle=True)\n",
        "test_loader_db2 = DataLoader(test_dataset_db2, batch_size=32, shuffle=False)\n",
        "\n",
        "static_input_dim = X_static_train_db2.shape[1]\n",
        "model2 = StaticModel(static_input_dim)\n",
        "\n",
        "# Define Loss and Optimizer\n",
        "num_positive = sum(y_train_db2)  # Positive samples\n",
        "num_negative = len(y_train_db2) - num_positive  # Negative samples\n",
        "pos_weight = num_negative / num_positive  # Weight for imbalance\n",
        "\n",
        "criterion = WeightedFocalLoss(pos_weight=pos_weight, gamma=1.5)\n",
        "optimizer = optim.Adam(model2.parameters(), lr=0.0005)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True)\n",
        "\n",
        "# Train the Model\n",
        "train_model(model2, train_loader_db2, test_loader_db2, criterion, optimizer, scheduler)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkexcNOv2DIA",
        "outputId": "75460122-c462-43eb-dbdf-8b0a1ce23540"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 0.2277, Recall: 0.6047, Precision: 0.5843, F2 Score: 0.6005, Accuracy: 0.5872\n",
            "Epoch [2/50], Loss: 0.1818, Recall: 0.7791, Precision: 0.6979, F2 Score: 0.7614, Accuracy: 0.7209\n",
            "Epoch [3/50], Loss: 0.1376, Recall: 0.5233, Precision: 0.7759, F2 Score: 0.5597, Accuracy: 0.6860\n",
            "Epoch [4/50], Loss: 0.1107, Recall: 0.5465, Precision: 0.7581, F2 Score: 0.5788, Accuracy: 0.6860\n",
            "Epoch [5/50], Loss: 0.0925, Recall: 0.5814, Precision: 0.8333, F2 Score: 0.6188, Accuracy: 0.7326\n",
            "Epoch [6/50], Loss: 0.0784, Recall: 0.6744, Precision: 0.7436, F2 Score: 0.6872, Accuracy: 0.7209\n",
            "Epoch [7/50], Loss: 0.0641, Recall: 0.6977, Precision: 0.8108, F2 Score: 0.7177, Accuracy: 0.7674\n",
            "Epoch [8/50], Loss: 0.0541, Recall: 0.8837, Precision: 0.6847, F2 Score: 0.8352, Accuracy: 0.7384\n",
            "Epoch [9/50], Loss: 0.0414, Recall: 0.7442, Precision: 0.7619, F2 Score: 0.7477, Accuracy: 0.7558\n",
            "Epoch [10/50], Loss: 0.0308, Recall: 0.7791, Precision: 0.7701, F2 Score: 0.7773, Accuracy: 0.7733\n",
            "Epoch [11/50], Loss: 0.0274, Recall: 0.8023, Precision: 0.7419, F2 Score: 0.7895, Accuracy: 0.7616\n",
            "Epoch [12/50], Loss: 0.0241, Recall: 0.6977, Precision: 0.8219, F2 Score: 0.7194, Accuracy: 0.7733\n",
            "Epoch [13/50], Loss: 0.0256, Recall: 0.7442, Precision: 0.7805, F2 Score: 0.7512, Accuracy: 0.7674\n",
            "Epoch [14/50], Loss: 0.0171, Recall: 0.8372, Precision: 0.7826, F2 Score: 0.8257, Accuracy: 0.8023\n",
            "Epoch [15/50], Loss: 0.0121, Recall: 0.7907, Precision: 0.7640, F2 Score: 0.7852, Accuracy: 0.7733\n",
            "Epoch [16/50], Loss: 0.0108, Recall: 0.7791, Precision: 0.7976, F2 Score: 0.7827, Accuracy: 0.7907\n",
            "Epoch [17/50], Loss: 0.0097, Recall: 0.8605, Precision: 0.7629, F2 Score: 0.8390, Accuracy: 0.7965\n",
            "Epoch [18/50], Loss: 0.0092, Recall: 0.8023, Precision: 0.7667, F2 Score: 0.7949, Accuracy: 0.7791\n",
            "Epoch [19/50], Loss: 0.0081, Recall: 0.8488, Precision: 0.7526, F2 Score: 0.8277, Accuracy: 0.7849\n",
            "Epoch [20/50], Loss: 0.0081, Recall: 0.8605, Precision: 0.7708, F2 Score: 0.8409, Accuracy: 0.8023\n",
            "Epoch [21/50], Loss: 0.0070, Recall: 0.8488, Precision: 0.7526, F2 Score: 0.8277, Accuracy: 0.7849\n",
            "Epoch [22/50], Loss: 0.0067, Recall: 0.7907, Precision: 0.7907, F2 Score: 0.7907, Accuracy: 0.7907\n",
            "Epoch [23/50], Loss: 0.0069, Recall: 0.8256, Precision: 0.7634, F2 Score: 0.8124, Accuracy: 0.7849\n",
            "Epoch [24/50], Loss: 0.0055, Recall: 0.8140, Precision: 0.7778, F2 Score: 0.8065, Accuracy: 0.7907\n",
            "Epoch [25/50], Loss: 0.0059, Recall: 0.8256, Precision: 0.7717, F2 Score: 0.8142, Accuracy: 0.7907\n",
            "Epoch [26/50], Loss: 0.0069, Recall: 0.8256, Precision: 0.7802, F2 Score: 0.8161, Accuracy: 0.7965\n",
            "Epoch [27/50], Loss: 0.0043, Recall: 0.7791, Precision: 0.7976, F2 Score: 0.7827, Accuracy: 0.7907\n",
            "Epoch [28/50], Loss: 0.0040, Recall: 0.8256, Precision: 0.7634, F2 Score: 0.8124, Accuracy: 0.7849\n",
            "Epoch [29/50], Loss: 0.0043, Recall: 0.8256, Precision: 0.7802, F2 Score: 0.8161, Accuracy: 0.7965\n",
            "Epoch [30/50], Loss: 0.0033, Recall: 0.8140, Precision: 0.8046, F2 Score: 0.8121, Accuracy: 0.8081\n",
            "Epoch [31/50], Loss: 0.0032, Recall: 0.8256, Precision: 0.7717, F2 Score: 0.8142, Accuracy: 0.7907\n",
            "Epoch [32/50], Loss: 0.0036, Recall: 0.8256, Precision: 0.7889, F2 Score: 0.8180, Accuracy: 0.8023\n",
            "Epoch [33/50], Loss: 0.0030, Recall: 0.8256, Precision: 0.7889, F2 Score: 0.8180, Accuracy: 0.8023\n",
            "Epoch [34/50], Loss: 0.0032, Recall: 0.8256, Precision: 0.7802, F2 Score: 0.8161, Accuracy: 0.7965\n",
            "Epoch [35/50], Loss: 0.0031, Recall: 0.8256, Precision: 0.7802, F2 Score: 0.8161, Accuracy: 0.7965\n",
            "Epoch [36/50], Loss: 0.0029, Recall: 0.8256, Precision: 0.7717, F2 Score: 0.8142, Accuracy: 0.7907\n",
            "Epoch [37/50], Loss: 0.0031, Recall: 0.8256, Precision: 0.7802, F2 Score: 0.8161, Accuracy: 0.7965\n",
            "Epoch [38/50], Loss: 0.0027, Recall: 0.8256, Precision: 0.7889, F2 Score: 0.8180, Accuracy: 0.8023\n",
            "Epoch [39/50], Loss: 0.0026, Recall: 0.8256, Precision: 0.7889, F2 Score: 0.8180, Accuracy: 0.8023\n",
            "Epoch [40/50], Loss: 0.0027, Recall: 0.8256, Precision: 0.7889, F2 Score: 0.8180, Accuracy: 0.8023\n",
            "Epoch [41/50], Loss: 0.0026, Recall: 0.8256, Precision: 0.7889, F2 Score: 0.8180, Accuracy: 0.8023\n",
            "Epoch [42/50], Loss: 0.0024, Recall: 0.8256, Precision: 0.7889, F2 Score: 0.8180, Accuracy: 0.8023\n",
            "Epoch [43/50], Loss: 0.0025, Recall: 0.8256, Precision: 0.7889, F2 Score: 0.8180, Accuracy: 0.8023\n",
            "Epoch [44/50], Loss: 0.0026, Recall: 0.8256, Precision: 0.7889, F2 Score: 0.8180, Accuracy: 0.8023\n",
            "Epoch [45/50], Loss: 0.0025, Recall: 0.8256, Precision: 0.7889, F2 Score: 0.8180, Accuracy: 0.8023\n",
            "Epoch [46/50], Loss: 0.0024, Recall: 0.8256, Precision: 0.7889, F2 Score: 0.8180, Accuracy: 0.8023\n",
            "Epoch [47/50], Loss: 0.0025, Recall: 0.8256, Precision: 0.7889, F2 Score: 0.8180, Accuracy: 0.8023\n",
            "Epoch [48/50], Loss: 0.0026, Recall: 0.8256, Precision: 0.7889, F2 Score: 0.8180, Accuracy: 0.8023\n",
            "Epoch [49/50], Loss: 0.0024, Recall: 0.8256, Precision: 0.7889, F2 Score: 0.8180, Accuracy: 0.8023\n",
            "Epoch [50/50], Loss: 0.0029, Recall: 0.8256, Precision: 0.7889, F2 Score: 0.8180, Accuracy: 0.8023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dynamic threshold\n",
        "from sklearn.metrics import precision_recall_curve, recall_score, precision_score, fbeta_score, accuracy_score\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "class StaticDataset(Dataset):\n",
        "    def __init__(self, static_data, target):\n",
        "        # Handle pandas DataFrame or NumPy array\n",
        "        if isinstance(static_data, pd.DataFrame):\n",
        "            self.static_data = torch.tensor(static_data.to_numpy(), dtype=torch.float32)\n",
        "        else:\n",
        "            self.static_data = torch.tensor(static_data, dtype=torch.float32)\n",
        "        self.target = torch.tensor(target, dtype=torch.float32)  # Convert to tensor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.target)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"static\": self.static_data[idx],\n",
        "            \"target\": self.target[idx],\n",
        "        }\n",
        "\n",
        "train_dataset_db2 = StaticDataset(X_static_train_db2, y_train_db2)\n",
        "test_dataset_db2 = StaticDataset(X_static_test_db2, y_test_db2)\n",
        "\n",
        "train_loader_db2 = DataLoader(train_dataset_db2, batch_size=32, shuffle=True)\n",
        "test_loader_db2 = DataLoader(test_dataset_db2, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the Static Model\n",
        "class StaticModel(nn.Module):\n",
        "    def __init__(self, static_input_dim):\n",
        "        super(StaticModel, self).__init__()\n",
        "\n",
        "        # Fully Connected Network for Static Features\n",
        "        self.static_fc = nn.Sequential(\n",
        "            nn.Linear(static_input_dim, 128),  # Increased neurons\n",
        "            nn.BatchNorm1d(128),  # Batch Normalization\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.BatchNorm1d(64),  # Batch Normalization\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.BatchNorm1d(32),  # Batch Normalization\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1),  # Final output layer\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, static_input):\n",
        "        return self.static_fc(static_input)\n",
        "\n",
        "\n",
        "# Initialize the Model, Loss, and Optimizer\n",
        "static_input_dim = X_static_train_db2.shape[1]\n",
        "\n",
        "model2 = StaticModel(static_input_dim)\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for binary classification\n",
        "optimizer = optim.Adam(model2.parameters(), lr=0.0005)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True)\n",
        "\n",
        "\n",
        "# Dynamic Threshold Finder\n",
        "def find_best_threshold(outputs, targets, beta=2, min_threshold=0.1):\n",
        "    precision, recall, thresholds = precision_recall_curve(targets, outputs)\n",
        "    fbeta_scores = (1 + beta**2) * (precision[:-1] * recall[:-1]) / ((beta**2 * precision[:-1]) + recall[:-1] + 1e-5)\n",
        "\n",
        "    valid_idx = thresholds >= min_threshold\n",
        "    thresholds = thresholds[valid_idx]\n",
        "    fbeta_scores = fbeta_scores[valid_idx]\n",
        "\n",
        "    if len(fbeta_scores) == 0:\n",
        "        return min_threshold, 0.0\n",
        "\n",
        "    best_idx = np.argmax(fbeta_scores)\n",
        "    return thresholds[best_idx], fbeta_scores[best_idx]\n",
        "\n",
        "\n",
        "# Training Model\n",
        "def train_model(model, train_loader, test_loader, criterion, optimizer, scheduler, epochs=50):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for batch in train_loader:\n",
        "            static_input = batch[\"static\"]\n",
        "            target = batch[\"target\"].unsqueeze(1)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(static_input)\n",
        "            loss = criterion(outputs, target)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Evaluate model and find the best threshold\n",
        "        outputs, targets, best_threshold, recall, precision, f2_score, accuracy = evaluate_model(model, test_loader)\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {running_loss / len(train_loader):.4f}, \"\n",
        "              f\"Best Threshold: {best_threshold:.4f}, Recall: {recall:.4f}, Precision: {precision:.4f}, \"\n",
        "              f\"F2 Score: {f2_score:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        # Step the scheduler\n",
        "        scheduler.step(f2_score)\n",
        "\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    all_outputs = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            static_input = batch[\"static\"]\n",
        "            target = batch[\"target\"]\n",
        "\n",
        "            outputs = model(static_input)\n",
        "            all_outputs.append(outputs.squeeze().numpy())\n",
        "            all_targets.append(target.numpy())\n",
        "\n",
        "    outputs = np.concatenate(all_outputs)\n",
        "    targets = np.concatenate(all_targets)\n",
        "\n",
        "    # Find the best dynamic threshold\n",
        "    best_threshold, best_fbeta = find_best_threshold(outputs, targets, beta=2)\n",
        "\n",
        "    # Apply the best threshold\n",
        "    predictions = (outputs > best_threshold).astype(int)\n",
        "    recall = recall_score(targets, predictions)\n",
        "    precision = precision_score(targets, predictions)\n",
        "    f2_score = fbeta_score(targets, predictions, beta=2)\n",
        "    accuracy = accuracy_score(targets, predictions)\n",
        "\n",
        "    return outputs, targets, best_threshold, recall, precision, f2_score, accuracy\n",
        "\n",
        "\n",
        "# Train the Static Model\n",
        "train_model(model2, train_loader_db2, test_loader_db2, criterion, optimizer, scheduler)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0su3a1q3kbc",
        "outputId": "32f00f76-8a03-4a8e-ba21-65ad6ecad377"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 0.6771, Best Threshold: 0.3908, Recall: 0.9651, Precision: 0.5425, F2 Score: 0.8350, Accuracy: 0.5756\n",
            "Epoch [2/50], Loss: 0.5926, Best Threshold: 0.3362, Recall: 0.9419, Precision: 0.5870, F2 Score: 0.8402, Accuracy: 0.6395\n",
            "Epoch [3/50], Loss: 0.5036, Best Threshold: 0.1483, Recall: 0.9767, Precision: 0.5600, F2 Score: 0.8502, Accuracy: 0.6047\n",
            "Epoch [4/50], Loss: 0.4515, Best Threshold: 0.1052, Recall: 0.9535, Precision: 0.5734, F2 Score: 0.8419, Accuracy: 0.6221\n",
            "Epoch [5/50], Loss: 0.4131, Best Threshold: 0.2046, Recall: 0.9767, Precision: 0.6000, F2 Score: 0.8678, Accuracy: 0.6628\n",
            "Epoch [6/50], Loss: 0.3534, Best Threshold: 0.1162, Recall: 0.9535, Precision: 0.5857, F2 Score: 0.8471, Accuracy: 0.6395\n",
            "Epoch [7/50], Loss: 0.3207, Best Threshold: 0.1515, Recall: 0.9767, Precision: 0.6043, F2 Score: 0.8696, Accuracy: 0.6686\n",
            "Epoch [8/50], Loss: 0.3023, Best Threshold: 0.2704, Recall: 0.9535, Precision: 0.6357, F2 Score: 0.8668, Accuracy: 0.7035\n",
            "Epoch [9/50], Loss: 0.2707, Best Threshold: 0.1089, Recall: 0.9767, Precision: 0.6176, F2 Score: 0.8750, Accuracy: 0.6860\n",
            "Epoch [10/50], Loss: 0.2216, Best Threshold: 0.1027, Recall: 0.9535, Precision: 0.6457, F2 Score: 0.8705, Accuracy: 0.7151\n",
            "Epoch [11/50], Loss: 0.2259, Best Threshold: 0.1138, Recall: 0.8953, Precision: 0.6638, F2 Score: 0.8370, Accuracy: 0.7209\n",
            "Epoch [12/50], Loss: 0.1887, Best Threshold: 0.1035, Recall: 0.8837, Precision: 0.6609, F2 Score: 0.8279, Accuracy: 0.7151\n",
            "Epoch [13/50], Loss: 0.1681, Best Threshold: 0.2552, Recall: 0.9302, Precision: 0.6897, F2 Score: 0.8696, Accuracy: 0.7558\n",
            "Epoch [14/50], Loss: 0.1403, Best Threshold: 0.1384, Recall: 0.9186, Precision: 0.6810, F2 Score: 0.8587, Accuracy: 0.7442\n",
            "Epoch [15/50], Loss: 0.1224, Best Threshold: 0.1238, Recall: 0.9302, Precision: 0.6780, F2 Score: 0.8658, Accuracy: 0.7442\n",
            "Epoch [16/50], Loss: 0.0922, Best Threshold: 0.1160, Recall: 0.8953, Precision: 0.7264, F2 Score: 0.8556, Accuracy: 0.7791\n",
            "Epoch [17/50], Loss: 0.0844, Best Threshold: 0.3111, Recall: 0.8721, Precision: 0.7732, F2 Score: 0.8503, Accuracy: 0.8081\n",
            "Epoch [18/50], Loss: 0.0769, Best Threshold: 0.3675, Recall: 0.8721, Precision: 0.7979, F2 Score: 0.8562, Accuracy: 0.8256\n",
            "Epoch [19/50], Loss: 0.0733, Best Threshold: 0.1075, Recall: 0.9535, Precision: 0.6667, F2 Score: 0.8779, Accuracy: 0.7384\n",
            "Epoch [20/50], Loss: 0.0667, Best Threshold: 0.1277, Recall: 0.9302, Precision: 0.6838, F2 Score: 0.8677, Accuracy: 0.7500\n",
            "Epoch [21/50], Loss: 0.0606, Best Threshold: 0.1056, Recall: 0.9186, Precision: 0.6930, F2 Score: 0.8624, Accuracy: 0.7558\n",
            "Epoch [22/50], Loss: 0.0589, Best Threshold: 0.1699, Recall: 0.8488, Precision: 0.7684, F2 Score: 0.8314, Accuracy: 0.7965\n",
            "Epoch [23/50], Loss: 0.0570, Best Threshold: 0.1729, Recall: 0.9302, Precision: 0.7080, F2 Score: 0.8753, Accuracy: 0.7733\n",
            "Epoch [24/50], Loss: 0.0504, Best Threshold: 0.1065, Recall: 0.9302, Precision: 0.6780, F2 Score: 0.8658, Accuracy: 0.7442\n",
            "Epoch [25/50], Loss: 0.0477, Best Threshold: 0.1257, Recall: 0.9419, Precision: 0.6694, F2 Score: 0.8710, Accuracy: 0.7384\n",
            "Epoch [26/50], Loss: 0.0389, Best Threshold: 0.1162, Recall: 0.8953, Precision: 0.7196, F2 Score: 0.8537, Accuracy: 0.7733\n",
            "Epoch [27/50], Loss: 0.0352, Best Threshold: 0.1052, Recall: 0.9419, Precision: 0.6532, F2 Score: 0.8654, Accuracy: 0.7209\n",
            "Epoch [28/50], Loss: 0.0401, Best Threshold: 0.2000, Recall: 0.8837, Precision: 0.7600, F2 Score: 0.8559, Accuracy: 0.8023\n",
            "Epoch [29/50], Loss: 0.0329, Best Threshold: 0.4138, Recall: 0.8721, Precision: 0.7895, F2 Score: 0.8542, Accuracy: 0.8198\n",
            "Epoch [30/50], Loss: 0.0321, Best Threshold: 0.3049, Recall: 0.8721, Precision: 0.8065, F2 Score: 0.8581, Accuracy: 0.8314\n",
            "Epoch [31/50], Loss: 0.0305, Best Threshold: 0.1562, Recall: 0.8837, Precision: 0.7677, F2 Score: 0.8578, Accuracy: 0.8081\n",
            "Epoch [32/50], Loss: 0.0280, Best Threshold: 0.1957, Recall: 0.8837, Precision: 0.7525, F2 Score: 0.8539, Accuracy: 0.7965\n",
            "Epoch [33/50], Loss: 0.0281, Best Threshold: 0.2641, Recall: 0.8837, Precision: 0.7600, F2 Score: 0.8559, Accuracy: 0.8023\n",
            "Epoch [34/50], Loss: 0.0258, Best Threshold: 0.2175, Recall: 0.8837, Precision: 0.7600, F2 Score: 0.8559, Accuracy: 0.8023\n",
            "Epoch [35/50], Loss: 0.0258, Best Threshold: 0.2293, Recall: 0.8837, Precision: 0.7600, F2 Score: 0.8559, Accuracy: 0.8023\n",
            "Epoch [36/50], Loss: 0.0251, Best Threshold: 0.1323, Recall: 0.8953, Precision: 0.7264, F2 Score: 0.8556, Accuracy: 0.7791\n",
            "Epoch [37/50], Loss: 0.0244, Best Threshold: 0.1074, Recall: 0.9070, Precision: 0.7091, F2 Score: 0.8590, Accuracy: 0.7674\n",
            "Epoch [38/50], Loss: 0.0231, Best Threshold: 0.2309, Recall: 0.8837, Precision: 0.7600, F2 Score: 0.8559, Accuracy: 0.8023\n",
            "Epoch [39/50], Loss: 0.0234, Best Threshold: 0.1936, Recall: 0.8837, Precision: 0.7600, F2 Score: 0.8559, Accuracy: 0.8023\n",
            "Epoch [40/50], Loss: 0.0229, Best Threshold: 0.1881, Recall: 0.8837, Precision: 0.7600, F2 Score: 0.8559, Accuracy: 0.8023\n",
            "Epoch [41/50], Loss: 0.0221, Best Threshold: 0.2292, Recall: 0.8837, Precision: 0.7677, F2 Score: 0.8578, Accuracy: 0.8081\n",
            "Epoch [42/50], Loss: 0.0221, Best Threshold: 0.2327, Recall: 0.8837, Precision: 0.7677, F2 Score: 0.8578, Accuracy: 0.8081\n",
            "Epoch [43/50], Loss: 0.0216, Best Threshold: 0.2014, Recall: 0.8837, Precision: 0.7600, F2 Score: 0.8559, Accuracy: 0.8023\n",
            "Epoch [44/50], Loss: 0.0207, Best Threshold: 0.2037, Recall: 0.8837, Precision: 0.7600, F2 Score: 0.8559, Accuracy: 0.8023\n",
            "Epoch [45/50], Loss: 0.0206, Best Threshold: 0.2261, Recall: 0.8837, Precision: 0.7677, F2 Score: 0.8578, Accuracy: 0.8081\n",
            "Epoch [46/50], Loss: 0.0209, Best Threshold: 0.2446, Recall: 0.8837, Precision: 0.7677, F2 Score: 0.8578, Accuracy: 0.8081\n",
            "Epoch [47/50], Loss: 0.0207, Best Threshold: 0.2100, Recall: 0.8837, Precision: 0.7677, F2 Score: 0.8578, Accuracy: 0.8081\n",
            "Epoch [48/50], Loss: 0.0203, Best Threshold: 0.2162, Recall: 0.8837, Precision: 0.7677, F2 Score: 0.8578, Accuracy: 0.8081\n",
            "Epoch [49/50], Loss: 0.0201, Best Threshold: 0.2420, Recall: 0.8837, Precision: 0.7600, F2 Score: 0.8559, Accuracy: 0.8023\n",
            "Epoch [50/50], Loss: 0.0200, Best Threshold: 0.2396, Recall: 0.8837, Precision: 0.7677, F2 Score: 0.8578, Accuracy: 0.8081\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 13qUlcJCRuQ4SjuMp80UTzqRdWZ6N9FTe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzEKzgj5i2ol",
        "outputId": "7ae9f329-fbae-4e35-b3b7-56dc8c942e63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=13qUlcJCRuQ4SjuMp80UTzqRdWZ6N9FTe\n",
            "To: /content/new_test_dataset_with_all_features.xlsx\n",
            "100% 238k/238k [00:00<00:00, 42.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fusion-WP"
      ],
      "metadata": {
        "id": "mli1G-CFngMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, fbeta_score, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# 加载测试数据\n",
        "test_data = pd.read_excel(\"new_test_dataset_with_all_features.xlsx\")\n",
        "\n",
        "# 使用标准化参数对静态特征标准化\n",
        "test_data[['年龄', '身高', '孕前BMI', '孕前体重']] = (test_data[['年龄', '身高', '孕前BMI', '孕前体重']] - mean_db1) / scale_db1\n",
        "\n",
        "# 处理 glu 和 weight 特征中的缺失值\n",
        "glu_features = test_data[['glu41', 'glu4b', 'glu4c', 'glu4d']].replace([' ', ''], np.nan).to_numpy()\n",
        "weight_features = test_data[['weight0', 'weight1', 'weight2', 'weight3', 'weight4']].replace([' ', ''], np.nan).to_numpy()\n",
        "\n",
        "glu_features = np.nan_to_num(glu_features.astype(np.float32), nan=0.0).reshape(len(glu_features), 4, 1)\n",
        "weight_features = np.nan_to_num(weight_features.astype(np.float32), nan=0.0).reshape(len(weight_features), 5, 1)\n",
        "\n",
        "# 准备模型1的静态特征\n",
        "model_static_features = test_data[['年龄', '居住地类别', '婚姻状况', '年收入分层', '学历分层', '工作状态',\n",
        "                                   '吸烟分层', '被动吸烟分层', '饮酒分层', '孕前体重', '身高', '孕前BMI']].to_numpy()\n",
        "\n",
        "# 使用模型2的标准化参数对静态特征进行标准化\n",
        "test_data[['年龄', '身高', '孕前BMI', '孕前体重']] = (test_data[['年龄', '身高', '孕前BMI', '孕前体重']] - mean_db2) / scale_db2\n",
        "model2_static_features = test_data[['年龄', '居住地类别', '婚姻状况', '年收入分层', '学历分层', '工作状态',\n",
        "                                    '吸烟分层', '被动吸烟分层', '饮酒分层', '孕前体重', '身高', '孕前BMI',\n",
        "                                    '意外怀孕', '助孕治疗', '已生孩子数分层', 'ACE1', 'ACE2', 'ACE3',\n",
        "                                    'ACE4', 'ACE5', 'ACE6', 'ACE7', 'ACE8', 'ACE9', 'ACE10', 'F1a',\n",
        "                                    'F2a', 'F3a', 'F4a', 'F5a', 'F6a', 'F7a', 'F8a', 'F9a', 'F10a',\n",
        "                                    'F11a', 'F12a', 'F13a', 'F14a', 'F15a', 'F16a', 'F17a', 'F18a',\n",
        "                                    'F19a']].to_numpy()\n",
        "\n",
        "# 模型1预测\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    static_tensor = torch.tensor(model_static_features, dtype=torch.float32)\n",
        "    glu_tensor = torch.tensor(glu_features, dtype=torch.float32)\n",
        "    weight_tensor = torch.tensor(weight_features, dtype=torch.float32)\n",
        "    outputs_model = model(static_tensor, glu_tensor, weight_tensor).squeeze().numpy()\n",
        "\n",
        "predictions_model = (outputs_model > 0.5).astype(int)  # 使用固定阈值\n",
        "\n",
        "# 模型2预测\n",
        "model2.eval()\n",
        "with torch.no_grad():\n",
        "    static_tensor_model2 = torch.tensor(model2_static_features, dtype=torch.float32)\n",
        "    outputs_model2 = model2(static_tensor_model2).squeeze().numpy()\n",
        "\n",
        "predictions_model2 = (outputs_model2 > 0.5).astype(int)  # 使用固定阈值\n",
        "\n",
        "\n",
        "\n",
        "final_predictions = []\n",
        "for output1, output2 in zip(outputs_model, outputs_model2):\n",
        "    # 根据加权概率确定最终预测\n",
        "    weighted_output = 0.75 * output1 + 0.25 * output2  # 调整权重比例\n",
        "    final_predictions.append(1 if weighted_output > 0.5 else 0)\n",
        "\n",
        "\n",
        "\n",
        "# 假设 test_data 中有真实标签列 \"target\"\n",
        "true_labels = test_data['target'].to_numpy()\n",
        "\n",
        "# 计算性能指标\n",
        "precision = precision_score(true_labels, final_predictions)\n",
        "recall = recall_score(true_labels, final_predictions)\n",
        "f2_score = fbeta_score(true_labels, final_predictions, beta=2)\n",
        "accuracy = accuracy_score(true_labels, final_predictions)\n",
        "\n",
        "# 打印结果\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F2 Score:\", f2_score)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BA9prqq66OTl",
        "outputId": "1ba6467b-5c57-4f01-e268-b3337b8063c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.5109489051094891\n",
            "Recall: 0.8187134502923976\n",
            "F2 Score: 0.7306889352818372\n",
            "Accuracy: 0.8012048192771084\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-96d6b5eae3ba>:14: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  glu_features = test_data[['glu41', 'glu4b', 'glu4c', 'glu4d']].replace([' ', ''], np.nan).to_numpy()\n",
            "<ipython-input-24-96d6b5eae3ba>:15: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  weight_features = test_data[['weight0', 'weight1', 'weight2', 'weight3', 'weight4']].replace([' ', ''], np.nan).to_numpy()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fusion-CIL"
      ],
      "metadata": {
        "id": "wHYO0wnDno8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, fbeta_score, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "test_data = pd.read_excel(\"new_test_dataset_with_all_features.xlsx\")\n",
        "# 手动输入的动态阈值\n",
        "model_best_threshold = 0.1190  # 训练好的模型1动态阈值\n",
        "model2_best_threshold = 0.2396  # 训练好的模型2动态阈值\n",
        "\n",
        "\n",
        "# Standardize static features using saved means and scales\n",
        "test_data[['年龄', '身高', '孕前BMI', '孕前体重']] = (test_data[['年龄', '身高', '孕前BMI', '孕前体重']] - mean_db1) / scale_db1\n",
        "\n",
        "# Handle missing values in glu and weight features\n",
        "glu_features = test_data[['glu41', 'glu4b', 'glu4c', 'glu4d']].replace([' ', ''], np.nan).to_numpy()\n",
        "weight_features = test_data[['weight0', 'weight1', 'weight2', 'weight3', 'weight4']].replace([' ', ''], np.nan).to_numpy()\n",
        "\n",
        "glu_features = np.nan_to_num(glu_features.astype(np.float32), nan=0.0).reshape(len(glu_features), 4, 1)\n",
        "weight_features = np.nan_to_num(weight_features.astype(np.float32), nan=0.0).reshape(len(weight_features), 5, 1)\n",
        "\n",
        "# Prepare static features for model\n",
        "model_static_features = test_data[['年龄', '居住地类别', '婚姻状况', '年收入分层', '学历分层', '工作状态',\n",
        "                                  '吸烟分层', '被动吸烟分层', '饮酒分层', '孕前体重', '身高', '孕前BMI']].to_numpy()\n",
        "\n",
        "# Standardize static features for model2 using its saved parameters\n",
        "test_data[['年龄', '身高', '孕前BMI', '孕前体重']] = (test_data[['年龄', '身高', '孕前BMI', '孕前体重']] - mean_db2) / scale_db2\n",
        "model2_static_features = test_data[['年龄', '居住地类别', '婚姻状况', '年收入分层', '学历分层', '工作状态',\n",
        "                                    '吸烟分层', '被动吸烟分层', '饮酒分层', '孕前体重', '身高', '孕前BMI',\n",
        "                                    '意外怀孕', '助孕治疗', '已生孩子数分层', 'ACE1', 'ACE2', 'ACE3',\n",
        "                                    'ACE4', 'ACE5', 'ACE6', 'ACE7', 'ACE8', 'ACE9', 'ACE10', 'F1a',\n",
        "                                    'F2a', 'F3a', 'F4a', 'F5a', 'F6a', 'F7a', 'F8a', 'F9a', 'F10a',\n",
        "                                    'F11a', 'F12a', 'F13a', 'F14a', 'F15a', 'F16a', 'F17a', 'F18a',\n",
        "                                    'F19a']].to_numpy()\n",
        "# 用模型1预测\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    static_tensor = torch.tensor(model_static_features, dtype=torch.float32)\n",
        "    glu_tensor = torch.tensor(glu_features, dtype=torch.float32)\n",
        "    weight_tensor = torch.tensor(weight_features, dtype=torch.float32)\n",
        "    outputs_model = model(static_tensor, glu_tensor, weight_tensor).squeeze().numpy()\n",
        "\n",
        "predictions_model = (outputs_model > model_best_threshold).astype(int)\n",
        "\n",
        "# 用模型2预测\n",
        "model2.eval()\n",
        "with torch.no_grad():\n",
        "    static_tensor_model2 = torch.tensor(model2_static_features, dtype=torch.float32)\n",
        "    outputs_model2 = model2(static_tensor_model2).squeeze().numpy()\n",
        "\n",
        "predictions_model2 = (outputs_model2 > model2_best_threshold).astype(int)\n",
        "\n",
        "# 最终预测\n",
        "final_predictions = []\n",
        "for pred1, pred2, output1, output2 in zip(predictions_model, predictions_model2, outputs_model, outputs_model2):\n",
        "    if pred1 == pred2:\n",
        "        final_predictions.append(pred1)\n",
        "    else:\n",
        "        # 使用置信区间更高的预测值\n",
        "        if abs(output1 - model_best_threshold) > abs(output2 - model2_best_threshold):\n",
        "            final_predictions.append(pred1)\n",
        "        else:\n",
        "            final_predictions.append(pred2)\n",
        "\n",
        "\n",
        "# 假设 test_data 中有真实标签列 \"真实标签\"\n",
        "true_labels = test_data['target'].to_numpy()\n",
        "\n",
        "# 计算性能指标\n",
        "precision = precision_score(true_labels, final_predictions)\n",
        "recall = recall_score(true_labels, final_predictions)\n",
        "f2_score = fbeta_score(true_labels, final_predictions, beta=2)\n",
        "accuracy = accuracy_score(true_labels, final_predictions)\n",
        "\n",
        "# 打印结果\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F2 Score:\", f2_score)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Model 1 Best Threshold:\", model_best_threshold)\n",
        "print(\"Model 2 Best Threshold:\", model2_best_threshold)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6wUCETTtti4",
        "outputId": "6385be34-00bd-42e7-ff93-66f8d1a05443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-ee78a2c19fec>:13: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  glu_features = test_data[['glu41', 'glu4b', 'glu4c', 'glu4d']].replace([' ', ''], np.nan).to_numpy()\n",
            "<ipython-input-9-ee78a2c19fec>:14: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  weight_features = test_data[['weight0', 'weight1', 'weight2', 'weight3', 'weight4']].replace([' ', ''], np.nan).to_numpy()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.8553459119496856\n",
            "Recall: 0.7953216374269005\n",
            "F2 Score: 0.8066429418742586\n",
            "Accuracy: 0.9301204819277108\n",
            "Model 1 Best Threshold: 0.119\n",
            "Model 2 Best Threshold: 0.2396\n"
          ]
        }
      ]
    }
  ]
}